{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd9c9f15",
   "metadata": {},
   "source": [
    "# Análise comparativa de modelos\n",
    "\n",
    "Esse notebook destina-se a uma análise comparativa de diferentes abordagens para predição de sentimento em tweets. O objetivo final é analisar diferentes combinações de modelos, vetorizadores e normalizadores e seus respectivos hiperparâmetros para definir dentro todas as combinações possíveis aquela que tenha uma melhor desempenho geral. Para garantir isso será efetuada uma validação cruzada para cada combinação possível tanto de modelos, quanto de hiperparâmetros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33666918",
   "metadata": {},
   "source": [
    "## Importando dependências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a9d945a-b822-4070-a5b7-e337c83c850c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/marvin-\n",
      "[nltk_data]     linux/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import (\n",
    "    CountVectorizer,\n",
    "    TfidfVectorizer,\n",
    ")\n",
    "from sklearn.model_selection import ShuffleSplit, RandomizedSearchCV, cross_validate\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn import svm\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "tweet_tokenizer = TweetTokenizer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5510de",
   "metadata": {},
   "source": [
    "## Importando dados (modificar) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79285eca-d73c-4258-a24c-93101f1bcee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(785814, 2)\n",
      "(695548, 3)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\n",
    "    \"../data/raw/NoThemeTweets.csv\", usecols=[\"tweet_text\", \"sentiment\"]\n",
    ")\n",
    "\n",
    "df = df.assign(\n",
    "    number_words=df.tweet_text.apply(lambda x: len(x.split(\" \"))),\n",
    ")  # adiciona coluna com número de palavras\n",
    "\n",
    "df.drop_duplicates([\"tweet_text\"], inplace=True)  # remove textos duplicados\n",
    "df.drop(\n",
    "    df[df.number_words < 5].index, inplace=True\n",
    ")  # remove tweets com menos de 5 palavras\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6823a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    \"data/processed\", usecols=[\"tweet_text\", \"sentiment\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17036efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df[\"tweet_text\"] = df[\"tweet_text\"].apply(\n",
    "    lambda tweet: formatar_texto(texto=tweet)\n",
    ")  # formata texto do dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45f3f48c-c8fa-4c21-ba89-161b5992cb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_raw = df.tweet_text.to_list()\n",
    "corpus_steamed = df.tweet_text.apply(lambda x: stemming(texto=x)).to_list()\n",
    "corpus_lammetized = df.tweet_text.apply(lambda x: lemmatization(text=x)).to_list()\n",
    "\n",
    "labels = df.sentiment.replace({\"Positivo\": 1, \"Negativo\": 0}).to_list()\n",
    "stop_words = nltk.corpus.stopwords.words(\"portuguese\") + [\"https\"] + [\"co\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e375a350",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1e44591",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_steamed[:10]\n",
    "\n",
    "corpus = {\n",
    "    \"corpus_raw\": {\n",
    "        \"corpus_data\": corpus_raw,\n",
    "    },\n",
    "    \"corpus_steamed\": {\n",
    "        \"corpus_data\": corpus_steamed,\n",
    "    },\n",
    "    \"corpus_lammetized\": {\n",
    "        \"corpus_data\": corpus_lammetized,\n",
    "    }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cceb73",
   "metadata": {},
   "source": [
    "### Inicialmente será definido um dicionário para cada uma  das etapas na formação de uma abordagem(pipeline), as etapas são:\n",
    "\n",
    "![\"Exemplo de pipeline\"](../Diagrama.png)\n",
    "\n",
    "\n",
    "Cada modelo, vetorizador e normalizador possui seu objeto e um conjunto de hiperparâmetros associados a ele. Para cada abordagem(vetorizador + normalizador + modelo) será feita uma validação cruzada, para garantir a consistência das métricas. Além disso, para garantir uma competição justa, cada abordagem deve ser otimizada com os melhores hiperparâmetros possíveis, para que todas estejam em sua melhor versão. Em vista disso, também é necessário utilizar uma validação cruzada neles.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93e5bb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"KNN\": {\n",
    "        \"model_obj\": KNeighborsClassifier(),\n",
    "        \"hyperparameters\": {\n",
    "            \"n_neighbors\": [7, 11, 21],\n",
    "            \"weights\": [\"uniform\", \"distance\"],\n",
    "        },\n",
    "    },\n",
    "    \"SMV\": {\n",
    "        \"model_obj\": svm.SVC(),\n",
    "        \"hyperparameters\": {\n",
    "            \"kernel\": [\"linear\", \"rbf\"],\n",
    "            \"C\": [0.1, 0.5, 1, 5, 10],\n",
    "        },\n",
    "    },\n",
    "    \"GaussianNB\": {\n",
    "        \"model_obj\": GaussianNB(),\n",
    "        \"hyperparameters\": {\n",
    "            \"var_smoothing\": [\n",
    "                1e-8,\n",
    "                1e-6,\n",
    "                1e-4,\n",
    "                1e-2,\n",
    "            ]\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "vectorizers = {\n",
    "    \"TfidfVectorizer\": {\n",
    "        \"vectorizer_obj\": TfidfVectorizer(),\n",
    "        \"hyperparameters\": {\n",
    "            \"max_features\": [500, 1000, 2000],\n",
    "            \"analyzer\": [\"word\", \"char\"],\n",
    "            \"stop_words\": [stop_words, None],\n",
    "            \"tokenizer\": [tweet_tokenizer.tokenize, None],\n",
    "        },\n",
    "    },\n",
    "    \"CountVectorizer\": {\n",
    "        \"vectorizer_obj\": CountVectorizer(),\n",
    "        \"hyperparameters\": {\n",
    "            \"max_features\": [500, 1000, 2000],\n",
    "            \"analyzer\": [\"word\", \"char\"],\n",
    "            \"stop_words\": [stop_words, None],\n",
    "            \"tokenizer\": [tweet_tokenizer.tokenize, None],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "normalizers = {\n",
    "    \"TrucatedSVD\": {\n",
    "        \"normalizer_obj\": TruncatedSVD(),\n",
    "        \"hyperparameters\": {\"n_components\": [3, 5, 10, 15]},\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a52c58",
   "metadata": {},
   "source": [
    "# Modelos selecionados:  \n",
    "\n",
    "- ### KNN(k-nearest neighbors):\n",
    "   O algoritmo KNN é um dos algoritmos clássicos de aprendizado de máquina, usualmente utilizado como algoritmo de classificação a ideia básica proposta  é que pontos semelhantes se encontra próximos um dos outros. Por se tratar de um algoritmo baseado na comparação de dados já existentes, o KNN é considerado um algoritmo do tipo \"preguiçoso\" já que basicamente decora os pontos do dataset, ou seja, o conhecimento já está diretamente nos dados e não em uma função preditora. No problema em questão sendo uma classificação binaria(positivo ou negativo) a classe definida será a que tiver mais de 50% dos votos. \n",
    "  \n",
    "- Hiperparâmetros:\n",
    "    - n_neighbors:\n",
    "      Número de vizinho próximos a ser analisado. A quantidade de pontos é geralmente definida como um número impar para evitar empates na classificação de um novo dado, após isso a classe com maior número de instâncias será a selecionada.\n",
    "      \n",
    "    - weights:\n",
    "      Define se a métrica utilizada será apenas a quantidade, ou se a distância dos pontos terá um peso.\n",
    "\n",
    "- ### SVC(Support Vector Classification).\n",
    "  O SVM funciona tentando criar uma hiperplano que separe linearmente os dados em classes diferentes, por exemplo, caso de uma plano 2d é simplesmente uma linha. O critério inicial para  isso é uma hiperplano é que ele consiga separar perfeitamente todos os dados, no caso de haver mais de um hiperplano que faça essa separação é definido como melhor aquele que maximiza a distância das instâncias de cada classe mais próxima. No caso dos dados não sejam linearmente separáveis a priore o SVM consegue aumentar quantidade de dimensões, tornando as classes separáveis dessa forma. No caso da análise de textos, montamos um vetor que represente aquele texto de alguma forma com n-dimensões para montar o hiperplano.\n",
    "\n",
    "  - Hiperparâmetros:\n",
    "    - kernel:\n",
    "    Kernel utilizado para o aumento da dimensionalidade do modelo. Usualmente para aplicações de NLP o linear costuma ser o melhor\n",
    "      \n",
    "    - C:\n",
    "    Parâmetro de regularização, \"afrouxa\" o critério de separação para ser possível separar mais facilmente os dados.\n",
    "\n",
    "- ### Gaussian Naive Bayes\n",
    "  O algoritmo Gausian Naive bayses consiste em fazer uma inferência baseado em várias curvas gaussianas adquiridas através das características do dataset de treino, onde cada uma delas é utilizada como uma parte para definir a probabilidade um dado ser de uma classe específica. No caso de um problema de NLP cada palavra possui sua curva gaussiana, associada com a probabilidade dela ser de uma classe ou outra. Em uma classificação binaria(positiva ou negativa), por exemplo, pode-se partir da pergunta: \"Esse texto é positivo?\" o algoritmo irá calcular a contagem de cada palavra presente no texto e repassar para cada curva gaussiana respectiva, no final irá tirar um score, a mesma coia será feita para a pergunta: \"Esse texto é negativo?\", calculando um novo score. Para a pergunta que obtiver o maior score será definida como a classe daquele novo input.\n",
    "  - Hiperparâmetros:\n",
    "    - var_smoothing:\n",
    "    Porção utilizada da maior variância, influencia diretamente na geração da curva.\n",
    "\n",
    "# Vetorizadores selecionados:  \n",
    "\n",
    "- ### CountVectorizer\n",
    "  Essa abordagem faz a contagem das palavras presente para cada uma das instâncias, no caso dessa aplicação tweets, as possibilidades são definidas baseadas no conjunto de todas as palavras possíveis de todos os tweets, o corpus. No final é gerado um vetor com a contagem de palavras presentes em cada tweet.\n",
    "\n",
    "- Hiperparâmetros:\n",
    "  - max_features:\n",
    "    Define a quantidade máxima de palavras que será mantido a contagem, no caso o algoritmo sempre priorizará as palavras que mais aparecem, pois, elas têm um maior peso para a definição da classe.\n",
    "  - analyzer:\n",
    "    Define se o algoritmo irá analisar palavra como features ou palavras.\n",
    "  - stop_words:\n",
    "    Define um conjunto de palavras ou não para ser removido dos textos antes da contagem, geralmente é removido palavras que não são relevantes para a análise.\n",
    "  - tokenizer:\n",
    "    Define o critério usado para separar as palavras no texto para serem contadas, dependendo da origem do texto pode melhorar muito a análise.\n",
    "    \n",
    "    \n",
    "- ### TfidfVectorizer\n",
    "  Essa abordagem faz a contagem das palavras por instância(tweets) assim como a CountVectorizer, porém além disso calcula a frequência que essa palavra apareceu baseado em todas as instâncias. Ou seja, uma palavra que aparece muito em um determinado tweet, mas muito pouco nos demais, terá um peso muito maior para a definição da classe daquele tweet. Do contrário, uma palavra que aparece em abundância,  em um tweet, mas é muito comum em todos os outros terá um peso menor.\n",
    "  \n",
    "- Hiperparâmetros:\n",
    "  - max_features:\n",
    "    Define a quantidade máxima de palavras que será mantido a contagem, no caso o algoritmo sempre priorizará as palavras que mais aparecem, pois, elas têm um maior peso para a definição da classe.\n",
    "  - analyzer:\n",
    "    Define se o algoritmo irá analisar palavra como features ou palavras.\n",
    "  - stop_words:\n",
    "    Define um conjunto de palavras ou não para ser removido dos textos antes da contagem, geralmente é removido palavras que não são relevantes para a análise.\n",
    "  - tokenizer:\n",
    "    Define o critério usado para separar as palavras no texto para serem contadas, dependendo da origem do texto pode melhorar muito a análise\n",
    "  \n",
    "\n",
    "# Normalizador selecionado:\n",
    "\n",
    "- ### TruncatedSVD()\n",
    "  Geralmente modelos que trabalham com NPL não lidam bem com vetores com uma grande quantidade de zeros seguidos, devido a numerosa quantidade de palavras possíveis dentro do corpus, as instâncias(tweets) não possuirão a maioria das palavras possíveis no corpus, gerando o problema citado acima. Para contornar isso é necessário reduzir para uma dimensão menor esses dados  que já foram filtrados anteriormente na contagem sendo os mais relevantes. Aplicando o redutor de dimensionalidade SVD, esse vetor espaçado com zeros será reduzido.\n",
    "\n",
    "- Hiperparâmetros:\n",
    "  - n_components: Define para quantas features o vetor será reduzido.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e4fb298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fiting best model to \n",
      "corpus_raw__KNN__TfidfVectorizer__TrucatedSVD\n",
      "\n",
      "Fiting best model to \n",
      "corpus_raw__KNN__CountVectorizer__TrucatedSVD\n",
      "\n",
      "Fiting best model to \n",
      "corpus_raw__SMV__TfidfVectorizer__TrucatedSVD\n",
      "\n",
      "Fiting best model to \n",
      "corpus_raw__SMV__CountVectorizer__TrucatedSVD\n",
      "\n",
      "Fiting best model to \n",
      "corpus_raw__GaussianNB__TfidfVectorizer__TrucatedSVD\n",
      "\n",
      "Fiting best model to \n",
      "corpus_raw__GaussianNB__CountVectorizer__TrucatedSVD\n",
      "\n",
      "Fiting best model to \n",
      "corpus_steamed__KNN__TfidfVectorizer__TrucatedSVD\n",
      "\n",
      "Fiting best model to \n",
      "corpus_steamed__KNN__CountVectorizer__TrucatedSVD\n",
      "\n",
      "Fiting best model to \n",
      "corpus_steamed__SMV__TfidfVectorizer__TrucatedSVD\n",
      "\n",
      "Fiting best model to \n",
      "corpus_steamed__SMV__CountVectorizer__TrucatedSVD\n",
      "\n",
      "Fiting best model to \n",
      "corpus_steamed__GaussianNB__TfidfVectorizer__TrucatedSVD\n",
      "\n",
      "Fiting best model to \n",
      "corpus_steamed__GaussianNB__CountVectorizer__TrucatedSVD\n",
      "\n",
      "Fiting best model to \n",
      "corpus_lammetized__KNN__TfidfVectorizer__TrucatedSVD\n",
      "\n",
      "Fiting best model to \n",
      "corpus_lammetized__KNN__CountVectorizer__TrucatedSVD\n",
      "\n",
      "Fiting best model to \n",
      "corpus_lammetized__SMV__TfidfVectorizer__TrucatedSVD\n",
      "\n",
      "Fiting best model to \n",
      "corpus_lammetized__SMV__CountVectorizer__TrucatedSVD\n",
      "\n",
      "Fiting best model to \n",
      "corpus_lammetized__GaussianNB__TfidfVectorizer__TrucatedSVD\n",
      "\n",
      "Fiting best model to \n",
      "corpus_lammetized__GaussianNB__CountVectorizer__TrucatedSVD\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_splits_cv = 1\n",
    "n_splits_gs = 5\n",
    "\n",
    "all_scores = {}\n",
    "\n",
    "split_cv = ShuffleSplit(n_splits= n_splits_cv, test_size=0.2, random_state = 42)\n",
    "\n",
    "for corpus_name, corpus_data in corpus.items():\n",
    "\n",
    "    for model_name, model_data in models.items():\n",
    "\n",
    "        model_params = {\n",
    "            f\"model__{key}\": value for key, value in model_data[\"hyperparameters\"].items()\n",
    "        }\n",
    "\n",
    "        for vectorizer_name, vectorizer_data in vectorizers.items():\n",
    "\n",
    "            vectorize_params = {\n",
    "                f\"vectorizer__{key}\": value\n",
    "                for key, value in vectorizer_data[\"hyperparameters\"].items()\n",
    "            }\n",
    "\n",
    "            for normalizer_name, normalizer_data in normalizers.items():\n",
    "\n",
    "                normalizer_params = {\n",
    "                    f\"normalizer__{key}\": value\n",
    "                    for key, value in normalizer_data[\"hyperparameters\"].items()\n",
    "                }\n",
    "\n",
    "                # for scaler_name, scaler_data in scalers.items():\n",
    "\n",
    "                #     scaler_params = {\n",
    "                #         f\"scaler__{key}\": value\n",
    "                #         for key, value in scaler_data[\"hyperparameters\"].items()\n",
    "                #     }\n",
    "\n",
    "                param_distributions = {\n",
    "                    **model_params,\n",
    "                    **vectorize_params,\n",
    "                    **normalizer_params,\n",
    "                    # **scaler_params,\n",
    "                }\n",
    "\n",
    "                pipeline = Pipeline(\n",
    "                    steps=[\n",
    "                        (\"vectorizer\", vectorizer_data[\"vectorizer_obj\"]),\n",
    "                        (\"normalizer\", normalizer_data[\"normalizer_obj\"]),\n",
    "                        # (\"scaler\", scaler_data[\"scaler_obj\"]),\n",
    "                        (\"model\", model_data[\"model_obj\"]),\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "                approach_name = f\"{corpus_name}__{model_name}__{vectorizer_name}__{normalizer_name}\"\n",
    "\n",
    "                print(f\"Fiting best model to \\n{approach_name}\", end=\"\\n\\n\")\n",
    "\n",
    "                tuned_pipeline = RandomizedSearchCV(\n",
    "                    pipeline,\n",
    "                    param_distributions,\n",
    "                    scoring=\"f1\",\n",
    "                    cv=n_splits_gs,\n",
    "                    random_state=42,\n",
    "                )\n",
    "\n",
    "                scores = cross_validate(\n",
    "                    tuned_pipeline,\n",
    "                    corpus_data[\"corpus_data\"],\n",
    "                    labels,\n",
    "                    cv= split_cv,\n",
    "                    scoring=[\"accuracy\", \"f1\", \"recall\"],\n",
    "                    ramdom_state=42,\n",
    "                )\n",
    "\n",
    "                all_scores.update(\n",
    "                    {\n",
    "                        approach_name: {\n",
    "                            \"scores\": scores,\n",
    "                            \"tuned_pipeline\": tuned_pipeline,\n",
    "                        }\n",
    "                    }\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19e49fd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'corpus_raw__KNN__TfidfVectorizer__TrucatedSVD': {'scores': {'fit_time': array([1.10153365, 0.7823174 ]),\n",
       "   'score_time': array([0.02262616, 0.01484323]),\n",
       "   'test_accuracy': array([1.        , 0.90607735]),\n",
       "   'test_f1': array([1.        , 0.84955752]),\n",
       "   'test_recall': array([1.        , 0.73846154])},\n",
       "  'tuned_pipeline': RandomizedSearchCV(cv=2,\n",
       "                     estimator=Pipeline(steps=[('vectorizer', TfidfVectorizer()),\n",
       "                                               ('normalizer', TruncatedSVD()),\n",
       "                                               ('model',\n",
       "                                                KNeighborsClassifier())]),\n",
       "                     param_distributions={'model__n_neighbors': [7, 11, 21],\n",
       "                                          'model__weights': ['uniform',\n",
       "                                                             'distance'],\n",
       "                                          'normalizer__n_components': [3, 5, 10,\n",
       "                                                                       15],\n",
       "                                          'vectorizer__analyzer': ['word',\n",
       "                                                                   'char'],\n",
       "                                          'vectorizer__max_features': [500,...\n",
       "                                          'vectorizer__stop_words': [['a', 'à',\n",
       "                                                                      'ao', 'aos',\n",
       "                                                                      'aquela',\n",
       "                                                                      'aquelas',\n",
       "                                                                      'aquele',\n",
       "                                                                      'aqueles',\n",
       "                                                                      'aquilo',\n",
       "                                                                      'as', 'às',\n",
       "                                                                      'até',\n",
       "                                                                      'com',\n",
       "                                                                      'como',\n",
       "                                                                      'da', 'das',\n",
       "                                                                      'de',\n",
       "                                                                      'dela',\n",
       "                                                                      'delas',\n",
       "                                                                      'dele',\n",
       "                                                                      'deles',\n",
       "                                                                      'depois',\n",
       "                                                                      'do', 'dos',\n",
       "                                                                      'e', 'é',\n",
       "                                                                      'ela',\n",
       "                                                                      'elas',\n",
       "                                                                      'ele',\n",
       "                                                                      'eles', ...],\n",
       "                                                                     None],\n",
       "                                          'vectorizer__tokenizer': [<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x7fa0a0210160>>,\n",
       "                                                                    None]},\n",
       "                     scoring='f1')},\n",
       " 'corpus_raw__KNN__CountVectorizer__TrucatedSVD': {'scores': {'fit_time': array([0.98026133, 0.83629227]),\n",
       "   'score_time': array([0.0269258 , 0.02357149]),\n",
       "   'test_accuracy': array([0.98342541, 0.97237569]),\n",
       "   'test_f1': array([0.97560976, 0.95652174]),\n",
       "   'test_recall': array([0.96774194, 0.94827586])},\n",
       "  'tuned_pipeline': RandomizedSearchCV(cv=2,\n",
       "                     estimator=Pipeline(steps=[('vectorizer', CountVectorizer()),\n",
       "                                               ('normalizer', TruncatedSVD()),\n",
       "                                               ('model',\n",
       "                                                KNeighborsClassifier())]),\n",
       "                     param_distributions={'model__n_neighbors': [7, 11, 21],\n",
       "                                          'model__weights': ['uniform',\n",
       "                                                             'distance'],\n",
       "                                          'normalizer__n_components': [3, 5, 10,\n",
       "                                                                       15],\n",
       "                                          'vectorizer__analyzer': ['word',\n",
       "                                                                   'char'],\n",
       "                                          'vectorizer__max_features': [500,...\n",
       "                                          'vectorizer__stop_words': [['a', 'à',\n",
       "                                                                      'ao', 'aos',\n",
       "                                                                      'aquela',\n",
       "                                                                      'aquelas',\n",
       "                                                                      'aquele',\n",
       "                                                                      'aqueles',\n",
       "                                                                      'aquilo',\n",
       "                                                                      'as', 'às',\n",
       "                                                                      'até',\n",
       "                                                                      'com',\n",
       "                                                                      'como',\n",
       "                                                                      'da', 'das',\n",
       "                                                                      'de',\n",
       "                                                                      'dela',\n",
       "                                                                      'delas',\n",
       "                                                                      'dele',\n",
       "                                                                      'deles',\n",
       "                                                                      'depois',\n",
       "                                                                      'do', 'dos',\n",
       "                                                                      'e', 'é',\n",
       "                                                                      'ela',\n",
       "                                                                      'elas',\n",
       "                                                                      'ele',\n",
       "                                                                      'eles', ...],\n",
       "                                                                     None],\n",
       "                                          'vectorizer__tokenizer': [<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x7fa0a0210160>>,\n",
       "                                                                    None]},\n",
       "                     scoring='f1')},\n",
       " 'corpus_raw__SMV__TfidfVectorizer__TrucatedSVD': {'scores': {'fit_time': array([1.08108091, 0.8689158 ]),\n",
       "   'score_time': array([0.02149224, 0.02148867]),\n",
       "   'test_accuracy': array([0.97790055, 0.96685083]),\n",
       "   'test_f1': array([0.96491228, 0.9516129 ]),\n",
       "   'test_recall': array([0.93220339, 0.9516129 ])},\n",
       "  'tuned_pipeline': RandomizedSearchCV(cv=2,\n",
       "                     estimator=Pipeline(steps=[('vectorizer', TfidfVectorizer()),\n",
       "                                               ('normalizer', TruncatedSVD()),\n",
       "                                               ('model', SVC())]),\n",
       "                     param_distributions={'model__C': [0.1, 0.5, 1, 5, 10],\n",
       "                                          'model__kernel': ['linear', 'rbf'],\n",
       "                                          'normalizer__n_components': [3, 5, 10,\n",
       "                                                                       15],\n",
       "                                          'vectorizer__analyzer': ['word',\n",
       "                                                                   'char'],\n",
       "                                          'vectorizer__max_features': [500, 1000,\n",
       "                                                                       2000],\n",
       "                                          'vectorizer__sto...['a', 'à',\n",
       "                                                                      'ao', 'aos',\n",
       "                                                                      'aquela',\n",
       "                                                                      'aquelas',\n",
       "                                                                      'aquele',\n",
       "                                                                      'aqueles',\n",
       "                                                                      'aquilo',\n",
       "                                                                      'as', 'às',\n",
       "                                                                      'até',\n",
       "                                                                      'com',\n",
       "                                                                      'como',\n",
       "                                                                      'da', 'das',\n",
       "                                                                      'de',\n",
       "                                                                      'dela',\n",
       "                                                                      'delas',\n",
       "                                                                      'dele',\n",
       "                                                                      'deles',\n",
       "                                                                      'depois',\n",
       "                                                                      'do', 'dos',\n",
       "                                                                      'e', 'é',\n",
       "                                                                      'ela',\n",
       "                                                                      'elas',\n",
       "                                                                      'ele',\n",
       "                                                                      'eles', ...],\n",
       "                                                                     None],\n",
       "                                          'vectorizer__tokenizer': [<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x7fa0a0210160>>,\n",
       "                                                                    None]},\n",
       "                     scoring='f1')},\n",
       " 'corpus_raw__SMV__CountVectorizer__TrucatedSVD': {'scores': {'fit_time': array([9.42374325, 1.95563793]),\n",
       "   'score_time': array([0.02103424, 0.02084112]),\n",
       "   'test_accuracy': array([0.98342541, 0.98342541]),\n",
       "   'test_f1': array([0.97297297, 0.97637795]),\n",
       "   'test_recall': array([1., 1.])},\n",
       "  'tuned_pipeline': RandomizedSearchCV(cv=2,\n",
       "                     estimator=Pipeline(steps=[('vectorizer', CountVectorizer()),\n",
       "                                               ('normalizer', TruncatedSVD()),\n",
       "                                               ('model', SVC())]),\n",
       "                     param_distributions={'model__C': [0.1, 0.5, 1, 5, 10],\n",
       "                                          'model__kernel': ['linear', 'rbf'],\n",
       "                                          'normalizer__n_components': [3, 5, 10,\n",
       "                                                                       15],\n",
       "                                          'vectorizer__analyzer': ['word',\n",
       "                                                                   'char'],\n",
       "                                          'vectorizer__max_features': [500, 1000,\n",
       "                                                                       2000],\n",
       "                                          'vectorizer__sto...['a', 'à',\n",
       "                                                                      'ao', 'aos',\n",
       "                                                                      'aquela',\n",
       "                                                                      'aquelas',\n",
       "                                                                      'aquele',\n",
       "                                                                      'aqueles',\n",
       "                                                                      'aquilo',\n",
       "                                                                      'as', 'às',\n",
       "                                                                      'até',\n",
       "                                                                      'com',\n",
       "                                                                      'como',\n",
       "                                                                      'da', 'das',\n",
       "                                                                      'de',\n",
       "                                                                      'dela',\n",
       "                                                                      'delas',\n",
       "                                                                      'dele',\n",
       "                                                                      'deles',\n",
       "                                                                      'depois',\n",
       "                                                                      'do', 'dos',\n",
       "                                                                      'e', 'é',\n",
       "                                                                      'ela',\n",
       "                                                                      'elas',\n",
       "                                                                      'ele',\n",
       "                                                                      'eles', ...],\n",
       "                                                                     None],\n",
       "                                          'vectorizer__tokenizer': [<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x7fa0a0210160>>,\n",
       "                                                                    None]},\n",
       "                     scoring='f1')},\n",
       " 'corpus_raw__GaussianNB__TfidfVectorizer__TrucatedSVD': {'scores': {'fit_time': array([1.81231284, 1.39972568]),\n",
       "   'score_time': array([0.02094626, 0.0214932 ]),\n",
       "   'test_accuracy': array([0.93370166, 0.96685083]),\n",
       "   'test_f1': array([0.89655172, 0.93181818]),\n",
       "   'test_recall': array([0.85245902, 0.87234043])},\n",
       "  'tuned_pipeline': RandomizedSearchCV(cv=2,\n",
       "                     estimator=Pipeline(steps=[('vectorizer', TfidfVectorizer()),\n",
       "                                               ('normalizer', TruncatedSVD()),\n",
       "                                               ('model', GaussianNB())]),\n",
       "                     param_distributions={'model__var_smoothing': [1e-08, 1e-06,\n",
       "                                                                   0.0001, 0.01],\n",
       "                                          'normalizer__n_components': [3, 5, 10,\n",
       "                                                                       15],\n",
       "                                          'vectorizer__analyzer': ['word',\n",
       "                                                                   'char'],\n",
       "                                          'vectorizer__max_features': [500, 1000,\n",
       "                                                                       2000],\n",
       "                                          'vectorizer__stop_wor...['a', 'à',\n",
       "                                                                      'ao', 'aos',\n",
       "                                                                      'aquela',\n",
       "                                                                      'aquelas',\n",
       "                                                                      'aquele',\n",
       "                                                                      'aqueles',\n",
       "                                                                      'aquilo',\n",
       "                                                                      'as', 'às',\n",
       "                                                                      'até',\n",
       "                                                                      'com',\n",
       "                                                                      'como',\n",
       "                                                                      'da', 'das',\n",
       "                                                                      'de',\n",
       "                                                                      'dela',\n",
       "                                                                      'delas',\n",
       "                                                                      'dele',\n",
       "                                                                      'deles',\n",
       "                                                                      'depois',\n",
       "                                                                      'do', 'dos',\n",
       "                                                                      'e', 'é',\n",
       "                                                                      'ela',\n",
       "                                                                      'elas',\n",
       "                                                                      'ele',\n",
       "                                                                      'eles', ...],\n",
       "                                                                     None],\n",
       "                                          'vectorizer__tokenizer': [<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x7fa0a0210160>>,\n",
       "                                                                    None]},\n",
       "                     scoring='f1')},\n",
       " 'corpus_raw__GaussianNB__CountVectorizer__TrucatedSVD': {'scores': {'fit_time': array([1.58454394, 2.36167192]),\n",
       "   'score_time': array([0.02487183, 0.02252626]),\n",
       "   'test_accuracy': array([0.93922652, 0.93922652]),\n",
       "   'test_f1': array([0.90598291, 0.89320388]),\n",
       "   'test_recall': array([0.9137931 , 0.85185185])},\n",
       "  'tuned_pipeline': RandomizedSearchCV(cv=2,\n",
       "                     estimator=Pipeline(steps=[('vectorizer', CountVectorizer()),\n",
       "                                               ('normalizer', TruncatedSVD()),\n",
       "                                               ('model', GaussianNB())]),\n",
       "                     param_distributions={'model__var_smoothing': [1e-08, 1e-06,\n",
       "                                                                   0.0001, 0.01],\n",
       "                                          'normalizer__n_components': [3, 5, 10,\n",
       "                                                                       15],\n",
       "                                          'vectorizer__analyzer': ['word',\n",
       "                                                                   'char'],\n",
       "                                          'vectorizer__max_features': [500, 1000,\n",
       "                                                                       2000],\n",
       "                                          'vectorizer__stop_wor...['a', 'à',\n",
       "                                                                      'ao', 'aos',\n",
       "                                                                      'aquela',\n",
       "                                                                      'aquelas',\n",
       "                                                                      'aquele',\n",
       "                                                                      'aqueles',\n",
       "                                                                      'aquilo',\n",
       "                                                                      'as', 'às',\n",
       "                                                                      'até',\n",
       "                                                                      'com',\n",
       "                                                                      'como',\n",
       "                                                                      'da', 'das',\n",
       "                                                                      'de',\n",
       "                                                                      'dela',\n",
       "                                                                      'delas',\n",
       "                                                                      'dele',\n",
       "                                                                      'deles',\n",
       "                                                                      'depois',\n",
       "                                                                      'do', 'dos',\n",
       "                                                                      'e', 'é',\n",
       "                                                                      'ela',\n",
       "                                                                      'elas',\n",
       "                                                                      'ele',\n",
       "                                                                      'eles', ...],\n",
       "                                                                     None],\n",
       "                                          'vectorizer__tokenizer': [<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x7fa0a0210160>>,\n",
       "                                                                    None]},\n",
       "                     scoring='f1')},\n",
       " 'corpus_steamed__KNN__TfidfVectorizer__TrucatedSVD': {'scores': {'fit_time': array([1.87958288, 1.25033855]),\n",
       "   'score_time': array([0.02971053, 0.00985956]),\n",
       "   'test_accuracy': array([0.93370166, 0.91712707]),\n",
       "   'test_f1': array([0.89473684, 0.85436893]),\n",
       "   'test_recall': array([0.87931034, 0.8       ])},\n",
       "  'tuned_pipeline': RandomizedSearchCV(cv=2,\n",
       "                     estimator=Pipeline(steps=[('vectorizer', TfidfVectorizer()),\n",
       "                                               ('normalizer', TruncatedSVD()),\n",
       "                                               ('model',\n",
       "                                                KNeighborsClassifier())]),\n",
       "                     param_distributions={'model__n_neighbors': [7, 11, 21],\n",
       "                                          'model__weights': ['uniform',\n",
       "                                                             'distance'],\n",
       "                                          'normalizer__n_components': [3, 5, 10,\n",
       "                                                                       15],\n",
       "                                          'vectorizer__analyzer': ['word',\n",
       "                                                                   'char'],\n",
       "                                          'vectorizer__max_features': [500,...\n",
       "                                          'vectorizer__stop_words': [['a', 'à',\n",
       "                                                                      'ao', 'aos',\n",
       "                                                                      'aquela',\n",
       "                                                                      'aquelas',\n",
       "                                                                      'aquele',\n",
       "                                                                      'aqueles',\n",
       "                                                                      'aquilo',\n",
       "                                                                      'as', 'às',\n",
       "                                                                      'até',\n",
       "                                                                      'com',\n",
       "                                                                      'como',\n",
       "                                                                      'da', 'das',\n",
       "                                                                      'de',\n",
       "                                                                      'dela',\n",
       "                                                                      'delas',\n",
       "                                                                      'dele',\n",
       "                                                                      'deles',\n",
       "                                                                      'depois',\n",
       "                                                                      'do', 'dos',\n",
       "                                                                      'e', 'é',\n",
       "                                                                      'ela',\n",
       "                                                                      'elas',\n",
       "                                                                      'ele',\n",
       "                                                                      'eles', ...],\n",
       "                                                                     None],\n",
       "                                          'vectorizer__tokenizer': [<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x7fa0a0210160>>,\n",
       "                                                                    None]},\n",
       "                     scoring='f1')},\n",
       " 'corpus_steamed__KNN__CountVectorizer__TrucatedSVD': {'scores': {'fit_time': array([1.7663753 , 1.63468504]),\n",
       "   'score_time': array([0.03310895, 0.02241564]),\n",
       "   'test_accuracy': array([0.98342541, 0.98895028]),\n",
       "   'test_f1': array([0.97345133, 0.98214286]),\n",
       "   'test_recall': array([0.96491228, 0.98214286])},\n",
       "  'tuned_pipeline': RandomizedSearchCV(cv=2,\n",
       "                     estimator=Pipeline(steps=[('vectorizer', CountVectorizer()),\n",
       "                                               ('normalizer', TruncatedSVD()),\n",
       "                                               ('model',\n",
       "                                                KNeighborsClassifier())]),\n",
       "                     param_distributions={'model__n_neighbors': [7, 11, 21],\n",
       "                                          'model__weights': ['uniform',\n",
       "                                                             'distance'],\n",
       "                                          'normalizer__n_components': [3, 5, 10,\n",
       "                                                                       15],\n",
       "                                          'vectorizer__analyzer': ['word',\n",
       "                                                                   'char'],\n",
       "                                          'vectorizer__max_features': [500,...\n",
       "                                          'vectorizer__stop_words': [['a', 'à',\n",
       "                                                                      'ao', 'aos',\n",
       "                                                                      'aquela',\n",
       "                                                                      'aquelas',\n",
       "                                                                      'aquele',\n",
       "                                                                      'aqueles',\n",
       "                                                                      'aquilo',\n",
       "                                                                      'as', 'às',\n",
       "                                                                      'até',\n",
       "                                                                      'com',\n",
       "                                                                      'como',\n",
       "                                                                      'da', 'das',\n",
       "                                                                      'de',\n",
       "                                                                      'dela',\n",
       "                                                                      'delas',\n",
       "                                                                      'dele',\n",
       "                                                                      'deles',\n",
       "                                                                      'depois',\n",
       "                                                                      'do', 'dos',\n",
       "                                                                      'e', 'é',\n",
       "                                                                      'ela',\n",
       "                                                                      'elas',\n",
       "                                                                      'ele',\n",
       "                                                                      'eles', ...],\n",
       "                                                                     None],\n",
       "                                          'vectorizer__tokenizer': [<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x7fa0a0210160>>,\n",
       "                                                                    None]},\n",
       "                     scoring='f1')},\n",
       " 'corpus_steamed__SMV__TfidfVectorizer__TrucatedSVD': {'scores': {'fit_time': array([2.04739022, 2.10337329]),\n",
       "   'score_time': array([0.03642344, 0.01279616]),\n",
       "   'test_accuracy': array([0.99447514, 0.92265193]),\n",
       "   'test_f1': array([0.99130435, 0.8627451 ]),\n",
       "   'test_recall': array([1.        , 0.81481481])},\n",
       "  'tuned_pipeline': RandomizedSearchCV(cv=2,\n",
       "                     estimator=Pipeline(steps=[('vectorizer', TfidfVectorizer()),\n",
       "                                               ('normalizer', TruncatedSVD()),\n",
       "                                               ('model', SVC())]),\n",
       "                     param_distributions={'model__C': [0.1, 0.5, 1, 5, 10],\n",
       "                                          'model__kernel': ['linear', 'rbf'],\n",
       "                                          'normalizer__n_components': [3, 5, 10,\n",
       "                                                                       15],\n",
       "                                          'vectorizer__analyzer': ['word',\n",
       "                                                                   'char'],\n",
       "                                          'vectorizer__max_features': [500, 1000,\n",
       "                                                                       2000],\n",
       "                                          'vectorizer__sto...['a', 'à',\n",
       "                                                                      'ao', 'aos',\n",
       "                                                                      'aquela',\n",
       "                                                                      'aquelas',\n",
       "                                                                      'aquele',\n",
       "                                                                      'aqueles',\n",
       "                                                                      'aquilo',\n",
       "                                                                      'as', 'às',\n",
       "                                                                      'até',\n",
       "                                                                      'com',\n",
       "                                                                      'como',\n",
       "                                                                      'da', 'das',\n",
       "                                                                      'de',\n",
       "                                                                      'dela',\n",
       "                                                                      'delas',\n",
       "                                                                      'dele',\n",
       "                                                                      'deles',\n",
       "                                                                      'depois',\n",
       "                                                                      'do', 'dos',\n",
       "                                                                      'e', 'é',\n",
       "                                                                      'ela',\n",
       "                                                                      'elas',\n",
       "                                                                      'ele',\n",
       "                                                                      'eles', ...],\n",
       "                                                                     None],\n",
       "                                          'vectorizer__tokenizer': [<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x7fa0a0210160>>,\n",
       "                                                                    None]},\n",
       "                     scoring='f1')},\n",
       " 'corpus_steamed__SMV__CountVectorizer__TrucatedSVD': {'scores': {'fit_time': array([2.05656838, 2.01615667]),\n",
       "   'score_time': array([0.02454829, 0.02382779]),\n",
       "   'test_accuracy': array([0.98342541, 0.97790055]),\n",
       "   'test_f1': array([0.97297297, 0.96825397]),\n",
       "   'test_recall': array([0.98181818, 0.98387097])},\n",
       "  'tuned_pipeline': RandomizedSearchCV(cv=2,\n",
       "                     estimator=Pipeline(steps=[('vectorizer', CountVectorizer()),\n",
       "                                               ('normalizer', TruncatedSVD()),\n",
       "                                               ('model', SVC())]),\n",
       "                     param_distributions={'model__C': [0.1, 0.5, 1, 5, 10],\n",
       "                                          'model__kernel': ['linear', 'rbf'],\n",
       "                                          'normalizer__n_components': [3, 5, 10,\n",
       "                                                                       15],\n",
       "                                          'vectorizer__analyzer': ['word',\n",
       "                                                                   'char'],\n",
       "                                          'vectorizer__max_features': [500, 1000,\n",
       "                                                                       2000],\n",
       "                                          'vectorizer__sto...['a', 'à',\n",
       "                                                                      'ao', 'aos',\n",
       "                                                                      'aquela',\n",
       "                                                                      'aquelas',\n",
       "                                                                      'aquele',\n",
       "                                                                      'aqueles',\n",
       "                                                                      'aquilo',\n",
       "                                                                      'as', 'às',\n",
       "                                                                      'até',\n",
       "                                                                      'com',\n",
       "                                                                      'como',\n",
       "                                                                      'da', 'das',\n",
       "                                                                      'de',\n",
       "                                                                      'dela',\n",
       "                                                                      'delas',\n",
       "                                                                      'dele',\n",
       "                                                                      'deles',\n",
       "                                                                      'depois',\n",
       "                                                                      'do', 'dos',\n",
       "                                                                      'e', 'é',\n",
       "                                                                      'ela',\n",
       "                                                                      'elas',\n",
       "                                                                      'ele',\n",
       "                                                                      'eles', ...],\n",
       "                                                                     None],\n",
       "                                          'vectorizer__tokenizer': [<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x7fa0a0210160>>,\n",
       "                                                                    None]},\n",
       "                     scoring='f1')},\n",
       " 'corpus_steamed__GaussianNB__TfidfVectorizer__TrucatedSVD': {'scores': {'fit_time': array([1.62030959, 1.65766883]),\n",
       "   'score_time': array([0.02300072, 0.02308679]),\n",
       "   'test_accuracy': array([0.89502762, 0.9558011 ]),\n",
       "   'test_f1': array([0.83478261, 0.93103448]),\n",
       "   'test_recall': array([0.76190476, 0.87096774])},\n",
       "  'tuned_pipeline': RandomizedSearchCV(cv=2,\n",
       "                     estimator=Pipeline(steps=[('vectorizer', TfidfVectorizer()),\n",
       "                                               ('normalizer', TruncatedSVD()),\n",
       "                                               ('model', GaussianNB())]),\n",
       "                     param_distributions={'model__var_smoothing': [1e-08, 1e-06,\n",
       "                                                                   0.0001, 0.01],\n",
       "                                          'normalizer__n_components': [3, 5, 10,\n",
       "                                                                       15],\n",
       "                                          'vectorizer__analyzer': ['word',\n",
       "                                                                   'char'],\n",
       "                                          'vectorizer__max_features': [500, 1000,\n",
       "                                                                       2000],\n",
       "                                          'vectorizer__stop_wor...['a', 'à',\n",
       "                                                                      'ao', 'aos',\n",
       "                                                                      'aquela',\n",
       "                                                                      'aquelas',\n",
       "                                                                      'aquele',\n",
       "                                                                      'aqueles',\n",
       "                                                                      'aquilo',\n",
       "                                                                      'as', 'às',\n",
       "                                                                      'até',\n",
       "                                                                      'com',\n",
       "                                                                      'como',\n",
       "                                                                      'da', 'das',\n",
       "                                                                      'de',\n",
       "                                                                      'dela',\n",
       "                                                                      'delas',\n",
       "                                                                      'dele',\n",
       "                                                                      'deles',\n",
       "                                                                      'depois',\n",
       "                                                                      'do', 'dos',\n",
       "                                                                      'e', 'é',\n",
       "                                                                      'ela',\n",
       "                                                                      'elas',\n",
       "                                                                      'ele',\n",
       "                                                                      'eles', ...],\n",
       "                                                                     None],\n",
       "                                          'vectorizer__tokenizer': [<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x7fa0a0210160>>,\n",
       "                                                                    None]},\n",
       "                     scoring='f1')},\n",
       " 'corpus_steamed__GaussianNB__CountVectorizer__TrucatedSVD': {'scores': {'fit_time': array([1.1356256 , 1.63805866]),\n",
       "   'score_time': array([0.00766635, 0.02167058]),\n",
       "   'test_accuracy': array([0.67403315, 0.89502762]),\n",
       "   'test_f1': array([0.28915663, 0.85496183]),\n",
       "   'test_recall': array([0.24      , 0.93333333])},\n",
       "  'tuned_pipeline': RandomizedSearchCV(cv=2,\n",
       "                     estimator=Pipeline(steps=[('vectorizer', CountVectorizer()),\n",
       "                                               ('normalizer', TruncatedSVD()),\n",
       "                                               ('model', GaussianNB())]),\n",
       "                     param_distributions={'model__var_smoothing': [1e-08, 1e-06,\n",
       "                                                                   0.0001, 0.01],\n",
       "                                          'normalizer__n_components': [3, 5, 10,\n",
       "                                                                       15],\n",
       "                                          'vectorizer__analyzer': ['word',\n",
       "                                                                   'char'],\n",
       "                                          'vectorizer__max_features': [500, 1000,\n",
       "                                                                       2000],\n",
       "                                          'vectorizer__stop_wor...['a', 'à',\n",
       "                                                                      'ao', 'aos',\n",
       "                                                                      'aquela',\n",
       "                                                                      'aquelas',\n",
       "                                                                      'aquele',\n",
       "                                                                      'aqueles',\n",
       "                                                                      'aquilo',\n",
       "                                                                      'as', 'às',\n",
       "                                                                      'até',\n",
       "                                                                      'com',\n",
       "                                                                      'como',\n",
       "                                                                      'da', 'das',\n",
       "                                                                      'de',\n",
       "                                                                      'dela',\n",
       "                                                                      'delas',\n",
       "                                                                      'dele',\n",
       "                                                                      'deles',\n",
       "                                                                      'depois',\n",
       "                                                                      'do', 'dos',\n",
       "                                                                      'e', 'é',\n",
       "                                                                      'ela',\n",
       "                                                                      'elas',\n",
       "                                                                      'ele',\n",
       "                                                                      'eles', ...],\n",
       "                                                                     None],\n",
       "                                          'vectorizer__tokenizer': [<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x7fa0a0210160>>,\n",
       "                                                                    None]},\n",
       "                     scoring='f1')},\n",
       " 'corpus_lammetized__KNN__TfidfVectorizer__TrucatedSVD': {'scores': {'fit_time': array([2.51097918, 2.13226962]),\n",
       "   'score_time': array([0.04503536, 0.0411005 ]),\n",
       "   'test_accuracy': array([0.96132597, 0.94475138]),\n",
       "   'test_f1': array([0.94488189, 0.90740741]),\n",
       "   'test_recall': array([0.92307692, 0.84482759])},\n",
       "  'tuned_pipeline': RandomizedSearchCV(cv=2,\n",
       "                     estimator=Pipeline(steps=[('vectorizer', TfidfVectorizer()),\n",
       "                                               ('normalizer', TruncatedSVD()),\n",
       "                                               ('model',\n",
       "                                                KNeighborsClassifier())]),\n",
       "                     param_distributions={'model__n_neighbors': [7, 11, 21],\n",
       "                                          'model__weights': ['uniform',\n",
       "                                                             'distance'],\n",
       "                                          'normalizer__n_components': [3, 5, 10,\n",
       "                                                                       15],\n",
       "                                          'vectorizer__analyzer': ['word',\n",
       "                                                                   'char'],\n",
       "                                          'vectorizer__max_features': [500,...\n",
       "                                          'vectorizer__stop_words': [['a', 'à',\n",
       "                                                                      'ao', 'aos',\n",
       "                                                                      'aquela',\n",
       "                                                                      'aquelas',\n",
       "                                                                      'aquele',\n",
       "                                                                      'aqueles',\n",
       "                                                                      'aquilo',\n",
       "                                                                      'as', 'às',\n",
       "                                                                      'até',\n",
       "                                                                      'com',\n",
       "                                                                      'como',\n",
       "                                                                      'da', 'das',\n",
       "                                                                      'de',\n",
       "                                                                      'dela',\n",
       "                                                                      'delas',\n",
       "                                                                      'dele',\n",
       "                                                                      'deles',\n",
       "                                                                      'depois',\n",
       "                                                                      'do', 'dos',\n",
       "                                                                      'e', 'é',\n",
       "                                                                      'ela',\n",
       "                                                                      'elas',\n",
       "                                                                      'ele',\n",
       "                                                                      'eles', ...],\n",
       "                                                                     None],\n",
       "                                          'vectorizer__tokenizer': [<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x7fa0a0210160>>,\n",
       "                                                                    None]},\n",
       "                     scoring='f1')},\n",
       " 'corpus_lammetized__KNN__CountVectorizer__TrucatedSVD': {'scores': {'fit_time': array([2.35332441, 2.27882981]),\n",
       "   'score_time': array([0.04550195, 0.05364299]),\n",
       "   'test_accuracy': array([0.97790055, 0.98342541]),\n",
       "   'test_f1': array([0.96610169, 0.97478992]),\n",
       "   'test_recall': array([0.96610169, 0.96666667])},\n",
       "  'tuned_pipeline': RandomizedSearchCV(cv=2,\n",
       "                     estimator=Pipeline(steps=[('vectorizer', CountVectorizer()),\n",
       "                                               ('normalizer', TruncatedSVD()),\n",
       "                                               ('model',\n",
       "                                                KNeighborsClassifier())]),\n",
       "                     param_distributions={'model__n_neighbors': [7, 11, 21],\n",
       "                                          'model__weights': ['uniform',\n",
       "                                                             'distance'],\n",
       "                                          'normalizer__n_components': [3, 5, 10,\n",
       "                                                                       15],\n",
       "                                          'vectorizer__analyzer': ['word',\n",
       "                                                                   'char'],\n",
       "                                          'vectorizer__max_features': [500,...\n",
       "                                          'vectorizer__stop_words': [['a', 'à',\n",
       "                                                                      'ao', 'aos',\n",
       "                                                                      'aquela',\n",
       "                                                                      'aquelas',\n",
       "                                                                      'aquele',\n",
       "                                                                      'aqueles',\n",
       "                                                                      'aquilo',\n",
       "                                                                      'as', 'às',\n",
       "                                                                      'até',\n",
       "                                                                      'com',\n",
       "                                                                      'como',\n",
       "                                                                      'da', 'das',\n",
       "                                                                      'de',\n",
       "                                                                      'dela',\n",
       "                                                                      'delas',\n",
       "                                                                      'dele',\n",
       "                                                                      'deles',\n",
       "                                                                      'depois',\n",
       "                                                                      'do', 'dos',\n",
       "                                                                      'e', 'é',\n",
       "                                                                      'ela',\n",
       "                                                                      'elas',\n",
       "                                                                      'ele',\n",
       "                                                                      'eles', ...],\n",
       "                                                                     None],\n",
       "                                          'vectorizer__tokenizer': [<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x7fa0a0210160>>,\n",
       "                                                                    None]},\n",
       "                     scoring='f1')},\n",
       " 'corpus_lammetized__SMV__TfidfVectorizer__TrucatedSVD': {'scores': {'fit_time': array([2.37012744, 2.67354441]),\n",
       "   'score_time': array([0.03484154, 0.03932142]),\n",
       "   'test_accuracy': array([0.99447514, 0.98342541]),\n",
       "   'test_f1': array([0.99065421, 0.97297297]),\n",
       "   'test_recall': array([0.98148148, 0.96428571])},\n",
       "  'tuned_pipeline': RandomizedSearchCV(cv=2,\n",
       "                     estimator=Pipeline(steps=[('vectorizer', TfidfVectorizer()),\n",
       "                                               ('normalizer', TruncatedSVD()),\n",
       "                                               ('model', SVC())]),\n",
       "                     param_distributions={'model__C': [0.1, 0.5, 1, 5, 10],\n",
       "                                          'model__kernel': ['linear', 'rbf'],\n",
       "                                          'normalizer__n_components': [3, 5, 10,\n",
       "                                                                       15],\n",
       "                                          'vectorizer__analyzer': ['word',\n",
       "                                                                   'char'],\n",
       "                                          'vectorizer__max_features': [500, 1000,\n",
       "                                                                       2000],\n",
       "                                          'vectorizer__sto...['a', 'à',\n",
       "                                                                      'ao', 'aos',\n",
       "                                                                      'aquela',\n",
       "                                                                      'aquelas',\n",
       "                                                                      'aquele',\n",
       "                                                                      'aqueles',\n",
       "                                                                      'aquilo',\n",
       "                                                                      'as', 'às',\n",
       "                                                                      'até',\n",
       "                                                                      'com',\n",
       "                                                                      'como',\n",
       "                                                                      'da', 'das',\n",
       "                                                                      'de',\n",
       "                                                                      'dela',\n",
       "                                                                      'delas',\n",
       "                                                                      'dele',\n",
       "                                                                      'deles',\n",
       "                                                                      'depois',\n",
       "                                                                      'do', 'dos',\n",
       "                                                                      'e', 'é',\n",
       "                                                                      'ela',\n",
       "                                                                      'elas',\n",
       "                                                                      'ele',\n",
       "                                                                      'eles', ...],\n",
       "                                                                     None],\n",
       "                                          'vectorizer__tokenizer': [<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x7fa0a0210160>>,\n",
       "                                                                    None]},\n",
       "                     scoring='f1')},\n",
       " 'corpus_lammetized__SMV__CountVectorizer__TrucatedSVD': {'scores': {'fit_time': array([13.68911862, 41.43588185]),\n",
       "   'score_time': array([0.04714823, 0.07136941]),\n",
       "   'test_accuracy': array([0.97237569, 0.98342541]),\n",
       "   'test_f1': array([0.96183206, 0.96969697]),\n",
       "   'test_recall': array([0.96923077, 0.94117647])},\n",
       "  'tuned_pipeline': RandomizedSearchCV(cv=2,\n",
       "                     estimator=Pipeline(steps=[('vectorizer', CountVectorizer()),\n",
       "                                               ('normalizer', TruncatedSVD()),\n",
       "                                               ('model', SVC())]),\n",
       "                     param_distributions={'model__C': [0.1, 0.5, 1, 5, 10],\n",
       "                                          'model__kernel': ['linear', 'rbf'],\n",
       "                                          'normalizer__n_components': [3, 5, 10,\n",
       "                                                                       15],\n",
       "                                          'vectorizer__analyzer': ['word',\n",
       "                                                                   'char'],\n",
       "                                          'vectorizer__max_features': [500, 1000,\n",
       "                                                                       2000],\n",
       "                                          'vectorizer__sto...['a', 'à',\n",
       "                                                                      'ao', 'aos',\n",
       "                                                                      'aquela',\n",
       "                                                                      'aquelas',\n",
       "                                                                      'aquele',\n",
       "                                                                      'aqueles',\n",
       "                                                                      'aquilo',\n",
       "                                                                      'as', 'às',\n",
       "                                                                      'até',\n",
       "                                                                      'com',\n",
       "                                                                      'como',\n",
       "                                                                      'da', 'das',\n",
       "                                                                      'de',\n",
       "                                                                      'dela',\n",
       "                                                                      'delas',\n",
       "                                                                      'dele',\n",
       "                                                                      'deles',\n",
       "                                                                      'depois',\n",
       "                                                                      'do', 'dos',\n",
       "                                                                      'e', 'é',\n",
       "                                                                      'ela',\n",
       "                                                                      'elas',\n",
       "                                                                      'ele',\n",
       "                                                                      'eles', ...],\n",
       "                                                                     None],\n",
       "                                          'vectorizer__tokenizer': [<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x7fa0a0210160>>,\n",
       "                                                                    None]},\n",
       "                     scoring='f1')},\n",
       " 'corpus_lammetized__GaussianNB__TfidfVectorizer__TrucatedSVD': {'scores': {'fit_time': array([3.00797915, 3.36707711]),\n",
       "   'score_time': array([0.02780795, 0.04941344]),\n",
       "   'test_accuracy': array([0.94475138, 0.90607735]),\n",
       "   'test_f1': array([0.90566038, 0.864     ]),\n",
       "   'test_recall': array([0.84210526, 0.83076923])},\n",
       "  'tuned_pipeline': RandomizedSearchCV(cv=2,\n",
       "                     estimator=Pipeline(steps=[('vectorizer', TfidfVectorizer()),\n",
       "                                               ('normalizer', TruncatedSVD()),\n",
       "                                               ('model', GaussianNB())]),\n",
       "                     param_distributions={'model__var_smoothing': [1e-08, 1e-06,\n",
       "                                                                   0.0001, 0.01],\n",
       "                                          'normalizer__n_components': [3, 5, 10,\n",
       "                                                                       15],\n",
       "                                          'vectorizer__analyzer': ['word',\n",
       "                                                                   'char'],\n",
       "                                          'vectorizer__max_features': [500, 1000,\n",
       "                                                                       2000],\n",
       "                                          'vectorizer__stop_wor...['a', 'à',\n",
       "                                                                      'ao', 'aos',\n",
       "                                                                      'aquela',\n",
       "                                                                      'aquelas',\n",
       "                                                                      'aquele',\n",
       "                                                                      'aqueles',\n",
       "                                                                      'aquilo',\n",
       "                                                                      'as', 'às',\n",
       "                                                                      'até',\n",
       "                                                                      'com',\n",
       "                                                                      'como',\n",
       "                                                                      'da', 'das',\n",
       "                                                                      'de',\n",
       "                                                                      'dela',\n",
       "                                                                      'delas',\n",
       "                                                                      'dele',\n",
       "                                                                      'deles',\n",
       "                                                                      'depois',\n",
       "                                                                      'do', 'dos',\n",
       "                                                                      'e', 'é',\n",
       "                                                                      'ela',\n",
       "                                                                      'elas',\n",
       "                                                                      'ele',\n",
       "                                                                      'eles', ...],\n",
       "                                                                     None],\n",
       "                                          'vectorizer__tokenizer': [<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x7fa0a0210160>>,\n",
       "                                                                    None]},\n",
       "                     scoring='f1')},\n",
       " 'corpus_lammetized__GaussianNB__CountVectorizer__TrucatedSVD': {'scores': {'fit_time': array([2.77805114, 2.1768496 ]),\n",
       "   'score_time': array([0.03513074, 0.01514125]),\n",
       "   'test_accuracy': array([0.8121547 , 0.67955801]),\n",
       "   'test_f1': array([0.75714286, 0.44230769]),\n",
       "   'test_recall': array([0.96363636, 0.37096774])},\n",
       "  'tuned_pipeline': RandomizedSearchCV(cv=2,\n",
       "                     estimator=Pipeline(steps=[('vectorizer', CountVectorizer()),\n",
       "                                               ('normalizer', TruncatedSVD()),\n",
       "                                               ('model', GaussianNB())]),\n",
       "                     param_distributions={'model__var_smoothing': [1e-08, 1e-06,\n",
       "                                                                   0.0001, 0.01],\n",
       "                                          'normalizer__n_components': [3, 5, 10,\n",
       "                                                                       15],\n",
       "                                          'vectorizer__analyzer': ['word',\n",
       "                                                                   'char'],\n",
       "                                          'vectorizer__max_features': [500, 1000,\n",
       "                                                                       2000],\n",
       "                                          'vectorizer__stop_wor...['a', 'à',\n",
       "                                                                      'ao', 'aos',\n",
       "                                                                      'aquela',\n",
       "                                                                      'aquelas',\n",
       "                                                                      'aquele',\n",
       "                                                                      'aqueles',\n",
       "                                                                      'aquilo',\n",
       "                                                                      'as', 'às',\n",
       "                                                                      'até',\n",
       "                                                                      'com',\n",
       "                                                                      'como',\n",
       "                                                                      'da', 'das',\n",
       "                                                                      'de',\n",
       "                                                                      'dela',\n",
       "                                                                      'delas',\n",
       "                                                                      'dele',\n",
       "                                                                      'deles',\n",
       "                                                                      'depois',\n",
       "                                                                      'do', 'dos',\n",
       "                                                                      'e', 'é',\n",
       "                                                                      'ela',\n",
       "                                                                      'elas',\n",
       "                                                                      'ele',\n",
       "                                                                      'eles', ...],\n",
       "                                                                     None],\n",
       "                                          'vectorizer__tokenizer': [<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x7fa0a0210160>>,\n",
       "                                                                    None]},\n",
       "                     scoring='f1')}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f7511e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_e4669_row0_col1, #T_e4669_row1_col1, #T_e4669_row2_col1, #T_e4669_row11_col2, #T_e4669_row11_col4, #T_e4669_row11_col5, #T_e4669_row17_col3 {\n",
       "  background-color: #fff7fb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e4669_row0_col2 {\n",
       "  background-color: #f1ebf5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e4669_row0_col3, #T_e4669_row12_col3 {\n",
       "  background-color: #045e93;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e4669_row0_col4, #T_e4669_row2_col5 {\n",
       "  background-color: #045c90;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e4669_row0_col5 {\n",
       "  background-color: #1e80b8;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e4669_row1_col2, #T_e4669_row17_col2 {\n",
       "  background-color: #d3d4e7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e4669_row1_col3, #T_e4669_row15_col3 {\n",
       "  background-color: #03446a;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e4669_row1_col4 {\n",
       "  background-color: #034267;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e4669_row1_col5 {\n",
       "  background-color: #045483;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e4669_row2_col2 {\n",
       "  background-color: #e6e2ef;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e4669_row2_col3 {\n",
       "  background-color: #034a74;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e4669_row2_col4 {\n",
       "  background-color: #03476f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e4669_row3_col1 {\n",
       "  background-color: #e0deed;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e4669_row3_col2 {\n",
       "  background-color: #e9e5f1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e4669_row3_col3 {\n",
       "  background-color: #023d60;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e4669_row3_col4 {\n",
       "  background-color: #023c5f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e4669_row3_col5, #T_e4669_row14_col3, #T_e4669_row14_col4, #T_e4669_row15_col1, #T_e4669_row15_col2 {\n",
       "  background-color: #023858;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e4669_row4_col1, #T_e4669_row6_col1 {\n",
       "  background-color: #fbf4f9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e4669_row4_col2 {\n",
       "  background-color: #e7e3f0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e4669_row4_col3 {\n",
       "  background-color: #046096;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e4669_row4_col4 {\n",
       "  background-color: #046198;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e4669_row4_col5 {\n",
       "  background-color: #2685bb;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e4669_row5_col1, #T_e4669_row9_col1 {\n",
       "  background-color: #f9f2f8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e4669_row5_col2 {\n",
       "  background-color: #dbdaeb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e4669_row5_col3 {\n",
       "  background-color: #0568a3;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e4669_row5_col4 {\n",
       "  background-color: #0567a2;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e4669_row5_col5 {\n",
       "  background-color: #1278b4;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e4669_row6_col2 {\n",
       "  background-color: #eee8f3;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e4669_row6_col3, #T_e4669_row10_col3, #T_e4669_row16_col3 {\n",
       "  background-color: #0872b1;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e4669_row6_col4 {\n",
       "  background-color: #0a73b2;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e4669_row6_col5 {\n",
       "  background-color: #3d93c2;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e4669_row7_col1, #T_e4669_row10_col1 {\n",
       "  background-color: #fbf3f9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e4669_row7_col2 {\n",
       "  background-color: #c1cae2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e4669_row7_col3, #T_e4669_row7_col4 {\n",
       "  background-color: #023a5b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e4669_row7_col5, #T_e4669_row14_col5 {\n",
       "  background-color: #034973;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e4669_row8_col1 {\n",
       "  background-color: #f8f1f8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e4669_row8_col2 {\n",
       "  background-color: #d6d6e9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e4669_row8_col3 {\n",
       "  background-color: #04598c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e4669_row8_col4, #T_e4669_row12_col4 {\n",
       "  background-color: #045b8f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e4669_row8_col5 {\n",
       "  background-color: #056ba9;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e4669_row9_col2 {\n",
       "  background-color: #d9d8ea;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e4669_row9_col3, #T_e4669_row13_col3 {\n",
       "  background-color: #034165;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e4669_row9_col4 {\n",
       "  background-color: #023e62;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e4669_row9_col5, #T_e4669_row15_col4 {\n",
       "  background-color: #034369;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e4669_row10_col2 {\n",
       "  background-color: #dedcec;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e4669_row10_col4 {\n",
       "  background-color: #056ead;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e4669_row10_col5 {\n",
       "  background-color: #589ec8;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e4669_row11_col1 {\n",
       "  background-color: #fdf5fa;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e4669_row11_col3 {\n",
       "  background-color: #e5e1ef;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e4669_row12_col1, #T_e4669_row13_col1 {\n",
       "  background-color: #f7f0f7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e4669_row12_col2 {\n",
       "  background-color: #308cbe;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e4669_row12_col5 {\n",
       "  background-color: #1077b4;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e4669_row13_col2 {\n",
       "  background-color: #056aa6;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e4669_row13_col4 {\n",
       "  background-color: #023f64;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e4669_row13_col5 {\n",
       "  background-color: #034d79;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e4669_row14_col1, #T_e4669_row17_col1 {\n",
       "  background-color: #f6eff7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e4669_row14_col2 {\n",
       "  background-color: #73a9cf;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e4669_row15_col5 {\n",
       "  background-color: #045585;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e4669_row16_col1 {\n",
       "  background-color: #f2ecf5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e4669_row16_col2 {\n",
       "  background-color: #62a2cb;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e4669_row16_col4 {\n",
       "  background-color: #056dac;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e4669_row16_col5 {\n",
       "  background-color: #4094c3;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e4669_row17_col4 {\n",
       "  background-color: #f5eef6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e4669_row17_col5 {\n",
       "  background-color: #dddbec;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_e4669\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_e4669_level0_col0\" class=\"col_heading level0 col0\" >approach</th>\n",
       "      <th id=\"T_e4669_level0_col1\" class=\"col_heading level0 col1\" >fit_time</th>\n",
       "      <th id=\"T_e4669_level0_col2\" class=\"col_heading level0 col2\" >score_time</th>\n",
       "      <th id=\"T_e4669_level0_col3\" class=\"col_heading level0 col3\" >accuracy</th>\n",
       "      <th id=\"T_e4669_level0_col4\" class=\"col_heading level0 col4\" >f1</th>\n",
       "      <th id=\"T_e4669_level0_col5\" class=\"col_heading level0 col5\" >recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_e4669_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_e4669_row0_col0\" class=\"data row0 col0\" >corpus_raw__KNN__TfidfVectorizer__TrucatedSVD</td>\n",
       "      <td id=\"T_e4669_row0_col1\" class=\"data row0 col1\" >0.941926</td>\n",
       "      <td id=\"T_e4669_row0_col2\" class=\"data row0 col2\" >0.018735</td>\n",
       "      <td id=\"T_e4669_row0_col3\" class=\"data row0 col3\" >0.953039</td>\n",
       "      <td id=\"T_e4669_row0_col4\" class=\"data row0 col4\" >0.924779</td>\n",
       "      <td id=\"T_e4669_row0_col5\" class=\"data row0 col5\" >0.869231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e4669_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_e4669_row1_col0\" class=\"data row1 col0\" >corpus_raw__KNN__CountVectorizer__TrucatedSVD</td>\n",
       "      <td id=\"T_e4669_row1_col1\" class=\"data row1 col1\" >0.908277</td>\n",
       "      <td id=\"T_e4669_row1_col2\" class=\"data row1 col2\" >0.025249</td>\n",
       "      <td id=\"T_e4669_row1_col3\" class=\"data row1 col3\" >0.977901</td>\n",
       "      <td id=\"T_e4669_row1_col4\" class=\"data row1 col4\" >0.966066</td>\n",
       "      <td id=\"T_e4669_row1_col5\" class=\"data row1 col5\" >0.958009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e4669_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_e4669_row2_col0\" class=\"data row2 col0\" >corpus_raw__SMV__TfidfVectorizer__TrucatedSVD</td>\n",
       "      <td id=\"T_e4669_row2_col1\" class=\"data row2 col1\" >0.974998</td>\n",
       "      <td id=\"T_e4669_row2_col2\" class=\"data row2 col2\" >0.021490</td>\n",
       "      <td id=\"T_e4669_row2_col3\" class=\"data row2 col3\" >0.972376</td>\n",
       "      <td id=\"T_e4669_row2_col4\" class=\"data row2 col4\" >0.958263</td>\n",
       "      <td id=\"T_e4669_row2_col5\" class=\"data row2 col5\" >0.941908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e4669_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_e4669_row3_col0\" class=\"data row3 col0\" >corpus_raw__SMV__CountVectorizer__TrucatedSVD</td>\n",
       "      <td id=\"T_e4669_row3_col1\" class=\"data row3 col1\" >5.689691</td>\n",
       "      <td id=\"T_e4669_row3_col2\" class=\"data row3 col2\" >0.020938</td>\n",
       "      <td id=\"T_e4669_row3_col3\" class=\"data row3 col3\" >0.983425</td>\n",
       "      <td id=\"T_e4669_row3_col4\" class=\"data row3 col4\" >0.974675</td>\n",
       "      <td id=\"T_e4669_row3_col5\" class=\"data row3 col5\" >1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e4669_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_e4669_row4_col0\" class=\"data row4 col0\" >corpus_raw__GaussianNB__TfidfVectorizer__TrucatedSVD</td>\n",
       "      <td id=\"T_e4669_row4_col1\" class=\"data row4 col1\" >1.606019</td>\n",
       "      <td id=\"T_e4669_row4_col2\" class=\"data row4 col2\" >0.021220</td>\n",
       "      <td id=\"T_e4669_row4_col3\" class=\"data row4 col3\" >0.950276</td>\n",
       "      <td id=\"T_e4669_row4_col4\" class=\"data row4 col4\" >0.914185</td>\n",
       "      <td id=\"T_e4669_row4_col5\" class=\"data row4 col5\" >0.862400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e4669_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_e4669_row5_col0\" class=\"data row5 col0\" >corpus_raw__GaussianNB__CountVectorizer__TrucatedSVD</td>\n",
       "      <td id=\"T_e4669_row5_col1\" class=\"data row5 col1\" >1.973108</td>\n",
       "      <td id=\"T_e4669_row5_col2\" class=\"data row5 col2\" >0.023699</td>\n",
       "      <td id=\"T_e4669_row5_col3\" class=\"data row5 col3\" >0.939227</td>\n",
       "      <td id=\"T_e4669_row5_col4\" class=\"data row5 col4\" >0.899593</td>\n",
       "      <td id=\"T_e4669_row5_col5\" class=\"data row5 col5\" >0.882822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e4669_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_e4669_row6_col0\" class=\"data row6 col0\" >corpus_steamed__KNN__TfidfVectorizer__TrucatedSVD</td>\n",
       "      <td id=\"T_e4669_row6_col1\" class=\"data row6 col1\" >1.564961</td>\n",
       "      <td id=\"T_e4669_row6_col2\" class=\"data row6 col2\" >0.019785</td>\n",
       "      <td id=\"T_e4669_row6_col3\" class=\"data row6 col3\" >0.925414</td>\n",
       "      <td id=\"T_e4669_row6_col4\" class=\"data row6 col4\" >0.874553</td>\n",
       "      <td id=\"T_e4669_row6_col5\" class=\"data row6 col5\" >0.839655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e4669_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_e4669_row7_col0\" class=\"data row7 col0\" >corpus_steamed__KNN__CountVectorizer__TrucatedSVD</td>\n",
       "      <td id=\"T_e4669_row7_col1\" class=\"data row7 col1\" >1.700530</td>\n",
       "      <td id=\"T_e4669_row7_col2\" class=\"data row7 col2\" >0.027762</td>\n",
       "      <td id=\"T_e4669_row7_col3\" class=\"data row7 col3\" >0.986188</td>\n",
       "      <td id=\"T_e4669_row7_col4\" class=\"data row7 col4\" >0.977797</td>\n",
       "      <td id=\"T_e4669_row7_col5\" class=\"data row7 col5\" >0.973528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e4669_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_e4669_row8_col0\" class=\"data row8 col0\" >corpus_steamed__SMV__TfidfVectorizer__TrucatedSVD</td>\n",
       "      <td id=\"T_e4669_row8_col1\" class=\"data row8 col1\" >2.075382</td>\n",
       "      <td id=\"T_e4669_row8_col2\" class=\"data row8 col2\" >0.024610</td>\n",
       "      <td id=\"T_e4669_row8_col3\" class=\"data row8 col3\" >0.958564</td>\n",
       "      <td id=\"T_e4669_row8_col4\" class=\"data row8 col4\" >0.927025</td>\n",
       "      <td id=\"T_e4669_row8_col5\" class=\"data row8 col5\" >0.907407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e4669_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_e4669_row9_col0\" class=\"data row9 col0\" >corpus_steamed__SMV__CountVectorizer__TrucatedSVD</td>\n",
       "      <td id=\"T_e4669_row9_col1\" class=\"data row9 col1\" >2.036363</td>\n",
       "      <td id=\"T_e4669_row9_col2\" class=\"data row9 col2\" >0.024188</td>\n",
       "      <td id=\"T_e4669_row9_col3\" class=\"data row9 col3\" >0.980663</td>\n",
       "      <td id=\"T_e4669_row9_col4\" class=\"data row9 col4\" >0.970613</td>\n",
       "      <td id=\"T_e4669_row9_col5\" class=\"data row9 col5\" >0.982845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e4669_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_e4669_row10_col0\" class=\"data row10 col0\" >corpus_steamed__GaussianNB__TfidfVectorizer__TrucatedSVD</td>\n",
       "      <td id=\"T_e4669_row10_col1\" class=\"data row10 col1\" >1.638989</td>\n",
       "      <td id=\"T_e4669_row10_col2\" class=\"data row10 col2\" >0.023044</td>\n",
       "      <td id=\"T_e4669_row10_col3\" class=\"data row10 col3\" >0.925414</td>\n",
       "      <td id=\"T_e4669_row10_col4\" class=\"data row10 col4\" >0.882909</td>\n",
       "      <td id=\"T_e4669_row10_col5\" class=\"data row10 col5\" >0.816436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e4669_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_e4669_row11_col0\" class=\"data row11 col0\" >corpus_steamed__GaussianNB__CountVectorizer__TrucatedSVD</td>\n",
       "      <td id=\"T_e4669_row11_col1\" class=\"data row11 col1\" >1.386842</td>\n",
       "      <td id=\"T_e4669_row11_col2\" class=\"data row11 col2\" >0.014668</td>\n",
       "      <td id=\"T_e4669_row11_col3\" class=\"data row11 col3\" >0.784530</td>\n",
       "      <td id=\"T_e4669_row11_col4\" class=\"data row11 col4\" >0.572059</td>\n",
       "      <td id=\"T_e4669_row11_col5\" class=\"data row11 col5\" >0.586667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e4669_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_e4669_row12_col0\" class=\"data row12 col0\" >corpus_lammetized__KNN__TfidfVectorizer__TrucatedSVD</td>\n",
       "      <td id=\"T_e4669_row12_col1\" class=\"data row12 col1\" >2.321624</td>\n",
       "      <td id=\"T_e4669_row12_col2\" class=\"data row12 col2\" >0.043068</td>\n",
       "      <td id=\"T_e4669_row12_col3\" class=\"data row12 col3\" >0.953039</td>\n",
       "      <td id=\"T_e4669_row12_col4\" class=\"data row12 col4\" >0.926145</td>\n",
       "      <td id=\"T_e4669_row12_col5\" class=\"data row12 col5\" >0.883952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e4669_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_e4669_row13_col0\" class=\"data row13 col0\" >corpus_lammetized__KNN__CountVectorizer__TrucatedSVD</td>\n",
       "      <td id=\"T_e4669_row13_col1\" class=\"data row13 col1\" >2.316077</td>\n",
       "      <td id=\"T_e4669_row13_col2\" class=\"data row13 col2\" >0.049572</td>\n",
       "      <td id=\"T_e4669_row13_col3\" class=\"data row13 col3\" >0.980663</td>\n",
       "      <td id=\"T_e4669_row13_col4\" class=\"data row13 col4\" >0.970446</td>\n",
       "      <td id=\"T_e4669_row13_col5\" class=\"data row13 col5\" >0.966384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e4669_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_e4669_row14_col0\" class=\"data row14 col0\" >corpus_lammetized__SMV__TfidfVectorizer__TrucatedSVD</td>\n",
       "      <td id=\"T_e4669_row14_col1\" class=\"data row14 col1\" >2.521836</td>\n",
       "      <td id=\"T_e4669_row14_col2\" class=\"data row14 col2\" >0.037081</td>\n",
       "      <td id=\"T_e4669_row14_col3\" class=\"data row14 col3\" >0.988950</td>\n",
       "      <td id=\"T_e4669_row14_col4\" class=\"data row14 col4\" >0.981814</td>\n",
       "      <td id=\"T_e4669_row14_col5\" class=\"data row14 col5\" >0.972884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e4669_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "      <td id=\"T_e4669_row15_col0\" class=\"data row15 col0\" >corpus_lammetized__SMV__CountVectorizer__TrucatedSVD</td>\n",
       "      <td id=\"T_e4669_row15_col1\" class=\"data row15 col1\" >27.562500</td>\n",
       "      <td id=\"T_e4669_row15_col2\" class=\"data row15 col2\" >0.059259</td>\n",
       "      <td id=\"T_e4669_row15_col3\" class=\"data row15 col3\" >0.977901</td>\n",
       "      <td id=\"T_e4669_row15_col4\" class=\"data row15 col4\" >0.965765</td>\n",
       "      <td id=\"T_e4669_row15_col5\" class=\"data row15 col5\" >0.955204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e4669_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "      <td id=\"T_e4669_row16_col0\" class=\"data row16 col0\" >corpus_lammetized__GaussianNB__TfidfVectorizer__TrucatedSVD</td>\n",
       "      <td id=\"T_e4669_row16_col1\" class=\"data row16 col1\" >3.187528</td>\n",
       "      <td id=\"T_e4669_row16_col2\" class=\"data row16 col2\" >0.038611</td>\n",
       "      <td id=\"T_e4669_row16_col3\" class=\"data row16 col3\" >0.925414</td>\n",
       "      <td id=\"T_e4669_row16_col4\" class=\"data row16 col4\" >0.884830</td>\n",
       "      <td id=\"T_e4669_row16_col5\" class=\"data row16 col5\" >0.836437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e4669_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "      <td id=\"T_e4669_row17_col0\" class=\"data row17 col0\" >corpus_lammetized__GaussianNB__CountVectorizer__TrucatedSVD</td>\n",
       "      <td id=\"T_e4669_row17_col1\" class=\"data row17 col1\" >2.477450</td>\n",
       "      <td id=\"T_e4669_row17_col2\" class=\"data row17 col2\" >0.025136</td>\n",
       "      <td id=\"T_e4669_row17_col3\" class=\"data row17 col3\" >0.745856</td>\n",
       "      <td id=\"T_e4669_row17_col4\" class=\"data row17 col4\" >0.599725</td>\n",
       "      <td id=\"T_e4669_row17_col5\" class=\"data row17 col5\" >0.667302</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fa044009dc0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_scores\n",
    "\n",
    "corpus_name = \"NoThemeTweets\"\n",
    "approach_names = []\n",
    "fit_times = []\n",
    "scores_times = []\n",
    "accuracy_means = []\n",
    "f1_scores_mean = []\n",
    "recall_scores_mean = []\n",
    "\n",
    "\n",
    "for approach_name, score in all_scores.items():\n",
    "    # print(f\"{approach_name}\")\n",
    "    # print(f\"{score['scores']}\")\n",
    "    approach_names.append(approach_name)\n",
    "    fit_times.append(score[\"scores\"][\"fit_time\"].mean())\n",
    "    scores_times.append(score[\"scores\"][\"score_time\"].mean())\n",
    "    accuracy_means.append(score[\"scores\"][\"test_accuracy\"].mean())\n",
    "    f1_scores_mean.append(score[\"scores\"][\"test_f1\"].mean())\n",
    "    recall_scores_mean.append(score[\"scores\"][\"test_recall\"].mean())\n",
    "    # print(\"\\n\")\n",
    "\n",
    "test_data = data = {\n",
    "    \"approach\": approach_names,\n",
    "    \"fit_time\": fit_times,\n",
    "    \"score_time\": scores_times,\n",
    "    \"accuracy\": accuracy_means,\n",
    "    \"f1\": f1_scores_mean,\n",
    "    \"recall\": recall_scores_mean,\n",
    "}\n",
    "\n",
    "\n",
    "test_data_df = pd.DataFrame(test_data)\n",
    "\n",
    "\n",
    "test_data_df.style.background_gradient()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "96f393ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "approach_names = test_data_df.approach\n",
    "\n",
    "import numpy as np\n",
    "def get_best_model(x):\n",
    "    if x.name.endswith(\"time\"):\n",
    "        return approach_names[np.argmin(x.values)]\n",
    "    \n",
    "    return approach_names[np.argmax(x.values)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "56dffedc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'corpus': 'corpus_lammetized',\n",
       " 'model': 'SMV',\n",
       " 'vectorizer': 'TfidfVectorizer',\n",
       " 'normalizer': 'TrucatedSVD'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from statistics import mode\n",
    "best_approach_name = mode(test_data_df.apply(get_best_model, axis=0).to_list()).split(\"__\")\n",
    "names = [\"corpus\", \"model\", \"vectorizer\", \"normalizer\"]\n",
    "\n",
    "best_approach_dict = {name: value for name , value in zip(names, best_approach_name)}\n",
    "\n",
    "best_approach_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "48933ca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer()"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizers\n",
    "\n",
    "normalizers[best_approach_dict[\"normalizer\"]][\"hyperparameters\"]\n",
    "\n",
    "\n",
    "vectorizers[best_approach_dict[\"vectorizer\"]][\"vectorizer_obj\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b68224",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "normalizer_params = {\n",
    "    f\"normalizer__{key}\": value\n",
    "    for key, value in normalizers[best_approach_dict[\"normalizer\"]][\"hyperparameters\"].items()\n",
    "}\n",
    "\n",
    "param_distributions = {\n",
    "    **model_params,\n",
    "    **vectorize_params,\n",
    "    **normalizer_params,\n",
    "    # **scaler_params,\n",
    "}\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"vectorizer\",  vectorizers[best_approach_dict[\"vectorizers\"]][\"vectorizer_obj\"]),\n",
    "        (\"normalizer\", normalizers[best_approach_dict[\"normalizer\"]][\"hyperparameters\"]),\n",
    "        # (\"scaler\", scaler_data[\"scaler_obj\"]),\n",
    "        (\"model\", model_data[\"model_obj\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "approach_name = f\"{corpus_name}__{model_name}__{vectorizer_name}__{normalizer_name}\"\n",
    "\n",
    "print(f\"Fiting best model to \\n{approach_name}\", end=\"\\n\\n\")\n",
    "\n",
    "tuned_pipeline = RandomizedSearchCV(\n",
    "    pipeline,\n",
    "    param_distributions,\n",
    "    scoring=\"f1\",\n",
    "    cv=n_splits_gs,\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e259b5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    df.tweet_text, df.sentiment, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "corpus_train2 = x_train.to_list()\n",
    "labels_train2 = y_train.replace({\"Positivo\": 1, \"Negativo\": 0}).to_list()\n",
    "\n",
    "corpus_test2 = x_test.to_list()\n",
    "labels_test2 = y_test.replace({\"Positivo\": 1, \"Negativo\": 0}).to_list()\n",
    "\n",
    "\n",
    "all_scores[\"SMV__TfidfVectorizer__PCA__Scaler\"][\"tuned_pipeline\"].fit(\n",
    "    corpus_train2, labels_train2\n",
    ")\n",
    "\n",
    "print(\"______\" * 30)\n",
    "print(all_scores[\"SMV__TfidfVectorizer__PCA__Scaler\"][\"tuned_pipeline\"].best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e0c7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = all_scores[\"SMV__TfidfVectorizer__PCA__Scaler\"][\"tuned_pipeline\"].predict(\n",
    "    corpus_test2\n",
    ")\n",
    "\n",
    "y_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f00716",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(labels_test2, y_hat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1761497",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(labels_test2, y_hat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79203b78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('src-zwQccrdW-py3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "882c8c6501720d7339a894262ca6c76e47c685bc126574a2b14e77f6df77b0f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
