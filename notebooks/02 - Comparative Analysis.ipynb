{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd9c9f15",
   "metadata": {},
   "source": [
    "# Análise comparativa de modelos\n",
    "\n",
    "### Esse notebook destina-se a uma análise comparativa de diferentes abordagens para predição de sentimento em tweets. O objetivo final é analisar diferentes combinações de modelos, vetorizadores e redutores e seus respectivos hiperparâmetros para definir dentro todas as combinações possíveis aquela que tenha uma melhor desempenho geral. Para garantir isso será efetuada uma validação cruzada para cada combinação possível tanto de modelos, quanto de hiperparâmetros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33666918",
   "metadata": {},
   "source": [
    "## Importando dependências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a9d945a-b822-4070-a5b7-e337c83c850c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Marvin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from joblib import dump\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from statistics import mode\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import ShuffleSplit, RandomizedSearchCV, cross_validate\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.feature_extraction.text import (\n",
    "    CountVectorizer,\n",
    "    TfidfVectorizer,\n",
    ")\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "tweet_tokenizer = TweetTokenizer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5510de",
   "metadata": {},
   "source": [
    "### Será utilizado quatro dataframes oriundos da etapa de pre processamento, conforme o pipeline abaixo:\n",
    "\n",
    "![\"pipeline formatação de texto\"](../notebooks/images/Pipeline_formata%C3%A7%C3%A3o_de_texto.png)\n",
    "\n",
    "\n",
    "## Importando dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f6823a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stemming_sem_stopwords = pd.read_csv(\n",
    "    \"../data/processed/df_steamed_no_stopwords.csv\",\n",
    "    usecols=[\"tweet_text\", \"sentiment\"],\n",
    ")\n",
    "\n",
    "df_stemming_com_stopwords = pd.read_csv(\n",
    "    \"../data/processed/df_steamed_with_stopwords.csv\",\n",
    "    usecols=[\"tweet_text\", \"sentiment\"],\n",
    ")\n",
    "\n",
    "df_lematizado_sem_stopwords = pd.read_csv(\n",
    "    \"../data/processed/df_lemmetized_no_stopwords.csv\",\n",
    "    usecols=[\"tweet_text\", \"sentiment\"],\n",
    ")\n",
    "\n",
    "df_lematizado_com_stopwords = pd.read_csv(\n",
    "    \"../data/processed/df_lemmetized_with_stopwords.csv\",\n",
    "    usecols=[\"tweet_text\", \"sentiment\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50f2b86",
   "metadata": {},
   "source": [
    "# Amostra de dados de cada dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "565da5eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>capaz q bom q pude ajudar com algo d</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>esse serviria para moderar o debate</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>o quantidade de cpf cancelar em 2019 ser Imens...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>otimo eu tbm vo de samus fazer um 4x só de samus</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>não ter nada para fazer</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  sentiment\n",
       "0               capaz q bom q pude ajudar com algo d          1\n",
       "1                esse serviria para moderar o debate          1\n",
       "2  o quantidade de cpf cancelar em 2019 ser Imens...          1\n",
       "3   otimo eu tbm vo de samus fazer um 4x só de samus          1\n",
       "4                            não ter nada para fazer          1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lematizado_sem_stopwords.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9e24a80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>capaz q bom q pude ajudar algo d</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>servir moderar debate</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>quantidade cpf cancelar 2019 imensar chego sal...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>otimo tbm vo samus fazer 4x samus</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nada fazer</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  sentiment\n",
       "0                   capaz q bom q pude ajudar algo d          1\n",
       "1                              servir moderar debate          1\n",
       "2  quantidade cpf cancelar 2019 imensar chego sal...          1\n",
       "3                  otimo tbm vo samus fazer 4x samus          1\n",
       "4                                         nada fazer          1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lematizado_com_stopwords.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33c414d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>capaz q bom q pude ajudar com algo d</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>esse serviria para moderar o debate</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>o quantidade de cpf cancelar em 2019 ser Imens...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>otimo eu tbm vo de samus fazer um 4x só de samus</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>não ter nada para fazer</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  sentiment\n",
       "0               capaz q bom q pude ajudar com algo d          1\n",
       "1                esse serviria para moderar o debate          1\n",
       "2  o quantidade de cpf cancelar em 2019 ser Imens...          1\n",
       "3   otimo eu tbm vo de samus fazer um 4x só de samus          1\n",
       "4                            não ter nada para fazer          1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lematizado_sem_stopwords.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14c309d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>capaz q bom q pud ajud alg d</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>serv moder debat</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>quant cpf cancel 2019 imens cheg saliv p</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>otim tbm vo samu faz 4x samu</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nad faz</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 tweet_text  sentiment\n",
       "0              capaz q bom q pud ajud alg d          1\n",
       "1                          serv moder debat          1\n",
       "2  quant cpf cancel 2019 imens cheg saliv p          1\n",
       "3              otim tbm vo samu faz 4x samu          1\n",
       "4                                   nad faz          1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stemming_com_stopwords.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdddea19",
   "metadata": {},
   "source": [
    "# Verificação de tamanho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d54326c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57966 57966 57966 57966\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    len(df_stemming_sem_stopwords),\n",
    "    len(df_stemming_com_stopwords),\n",
    "    len(df_lematizado_sem_stopwords),\n",
    "    len(df_lematizado_com_stopwords),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629a8e6d",
   "metadata": {},
   "source": [
    "# Separação dos textos do dataset e suas respectivas labels\n",
    "\n",
    "### Positivo = 1\n",
    "### Negativo  = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45f3f48c-c8fa-4c21-ba89-161b5992cb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_stemming_sem_stopwords = df_stemming_sem_stopwords.tweet_text.apply(\n",
    "    lambda tweet_text: np.str_(tweet_text)\n",
    ")\n",
    "\n",
    "corpus_stemming_com_stopwords = df_stemming_com_stopwords.tweet_text.apply(\n",
    "    lambda tweet_text: np.str_(tweet_text)\n",
    ")\n",
    "corpus_lematizado_sem_stopwords = df_lematizado_sem_stopwords.tweet_text.apply(\n",
    "    lambda tweet_text: np.str_(tweet_text)\n",
    ")\n",
    "corpus_lematizado_com_stopwords = df_lematizado_com_stopwords.tweet_text.apply(\n",
    "    lambda tweet_text: np.str_(tweet_text)\n",
    ")\n",
    "\n",
    "labels_stemming_sem_stopwords = df_stemming_sem_stopwords.sentiment.replace(\n",
    "    {\"Positivo\": 1, \"Negativo\": 0}\n",
    ").to_list()\n",
    "\n",
    "labels_stemming_com_stopwords = df_stemming_com_stopwords.sentiment.replace(\n",
    "    {\"Positivo\": 1, \"Negativo\": 0}\n",
    ").to_list()\n",
    "\n",
    "labels__lematizado_sem_stopwords = df_lematizado_sem_stopwords.sentiment.replace(\n",
    "    {\"Positivo\": 1, \"Negativo\": 0}\n",
    ").to_list()\n",
    "\n",
    "labels_lematizado_com_stopwords = df_lematizado_com_stopwords.sentiment.replace(\n",
    "    {\"Positivo\": 1, \"Negativo\": 0}\n",
    ").to_list()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d1ca88",
   "metadata": {},
   "source": [
    "# Verificação do tamanho do vetor de tweets e labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e375a350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57966 57966 57966 57966 57966 57966 57966 57966\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    len(corpus_stemming_sem_stopwords),\n",
    "    len(corpus_stemming_com_stopwords),\n",
    "    len(corpus_lematizado_sem_stopwords),\n",
    "    len(corpus_lematizado_com_stopwords),\n",
    "    len(labels_stemming_sem_stopwords),\n",
    "    len(labels_stemming_com_stopwords),\n",
    "    len(labels__lematizado_sem_stopwords),\n",
    "    len(labels_lematizado_com_stopwords),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cceb73",
   "metadata": {},
   "source": [
    "### Inicialmente será definido um dicionário para cada uma  das etapas na formação de uma abordagem(pipeline), as etapas são:\n",
    "\n",
    "![\"pipeline approach\"](../notebooks/images/pipeline_approach.png)\n",
    "\n",
    "Cada modelo, vetorizador e redutores possui seu objeto e um conjunto de hiperparâmetros associados a ele. Para cada abordagem(vetorizador + redutor + modelo) será feita uma validação cruzada, para garantir a consistência das métricas. Além disso, para garantir uma competição justa, cada abordagem deve ser otimizada com os melhores hiperparâmetros possíveis, para que todas estejam em sua melhor versão. Em vista disso, também é necessário utilizar uma validação cruzada neles.\n",
    "\n",
    "### A Definição dos componentes é feita abaixo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93e5bb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"KNN\": {\n",
    "        \"model_obj\": KNeighborsClassifier(),\n",
    "        \"hyperparameters\": {\n",
    "            \"n_neighbors\": [7, 11, 21],\n",
    "            \"weights\": [\"uniform\", \"distance\"],\n",
    "        },\n",
    "    },\n",
    "    \"SMV\": {\n",
    "        \"model_obj\": LinearSVC(max_iter=15000),\n",
    "        \"hyperparameters\": {\n",
    "            \"C\": [0.1, 1, 10, 100, 200],\n",
    "            \"random_state\": [42],\n",
    "        },\n",
    "    },\n",
    "    \"GaussianNB\": {\n",
    "        \"model_obj\": GaussianNB(),\n",
    "        \"hyperparameters\": {\n",
    "            \"var_smoothing\": [\n",
    "                1e-8,\n",
    "                1e-6,\n",
    "                1e-4,\n",
    "                1e-2,\n",
    "                1e-1,\n",
    "            ]\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "vectorizers = {\n",
    "    \"TfidfVectorizer\": {\n",
    "        \"vectorizer_obj\": TfidfVectorizer(),\n",
    "        \"hyperparameters\": {\n",
    "            \"max_features\": [250, 500, 1000, 2000],\n",
    "            \"analyzer\": [\"word\", \"char\"],\n",
    "            \"tokenizer\": [tweet_tokenizer.tokenize, None],\n",
    "        },\n",
    "    },\n",
    "    \"CountVectorizer\": {\n",
    "        \"vectorizer_obj\": CountVectorizer(),\n",
    "        \"hyperparameters\": {\n",
    "            \"max_features\": [250, 500, 1000, 2000],\n",
    "            \"analyzer\": [\"word\", \"char\"],\n",
    "            \"tokenizer\": [tweet_tokenizer.tokenize, None],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "reducer = {\n",
    "    \"TrucatedSVD\": {\n",
    "        \"reducer_obj\": TruncatedSVD(),\n",
    "        \"hyperparameters\": {\n",
    "            \"n_components\": [\n",
    "                100,\n",
    "                150,\n",
    "                200,\n",
    "            ]\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "corpus = {\n",
    "    \"corpus_stemming_sem_stopwords\": {\n",
    "        \"corpus_data\": corpus_stemming_sem_stopwords,\n",
    "        \"corpus_labels\": labels_stemming_sem_stopwords,\n",
    "    },\n",
    "    \"corpus_stemming_com_stopwords\": {\n",
    "        \"corpus_data\": corpus_stemming_com_stopwords,\n",
    "        \"corpus_labels\": labels_stemming_com_stopwords,\n",
    "    },\n",
    "    \"corpus_lematizado_sem_stopwords\": {\n",
    "        \"corpus_data\": corpus_lematizado_sem_stopwords,\n",
    "        \"corpus_labels\": labels__lematizado_sem_stopwords,\n",
    "    },\n",
    "    \"corpus_lematizado_com_stopwords\": {\n",
    "        \"corpus_data\": corpus_lematizado_com_stopwords,\n",
    "        \"corpus_labels\": labels_lematizado_com_stopwords,\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa0fd28",
   "metadata": {},
   "source": [
    "# Abaixo para cada elemento utilizado foi feita uma descrição simplificado do seu funcionamento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a52c58",
   "metadata": {},
   "source": [
    "# Modelos selecionados:  \n",
    "\n",
    "- ### KNN (k-nearest neighbors):\n",
    "   O algoritmo KNN é um dos algoritmos clássicos de aprendizado de máquina, usualmente utilizado como algoritmo de classificação a ideia básica proposta é que pontos semelhantes se encontra próximos um dos outros. Por se tratar de um algoritmo baseado na comparação de dados já existentes, o KNN é considerado um algoritmo do tipo \"preguiçoso\" já que basicamente decora os pontos do dataset, ou seja, o conhecimento já está diretamente nos dados e não em uma função preditora. No problema em questão sendo uma classificação binaria(positivo ou negativo) a classe definida será a que tiver mais de 50% dos votos. \n",
    "  \n",
    "- Hiperparâmetros:\n",
    "    - n_neighbors:\n",
    "      Número de vizinho próximos a ser analisado. A quantidade de pontos é geralmente definida como um número impar para evitar empates na classificação de um novo dado, após isso a classe com maior número de instâncias será a selecionada.\n",
    "      \n",
    "    - weights:\n",
    "      Define se a métrica utilizada será apenas a quantidade, ou se a distância dos pontos terá um peso.\n",
    "\n",
    "- ### SVC (Support Vector Classification).\n",
    "  O SVM funciona tentando criar uma hiperplano que separe linearmente os dados em classes diferentes, por exemplo, caso de uma plano 2d é simplesmente uma linha. O critério inicial para  isso é uma hiperplano é que ele consiga separar perfeitamente todos os dados, no caso de haver mais de um hiperplano que faça essa separação é definido como melhor aquele que maximiza a distância das instâncias de cada classe mais próxima. No caso dos dados não sejam linearmente separáveis a priore o SVM consegue aumentar quantidade de dimensões, tornando as classes separáveis dessa forma. No caso da análise de textos, montamos um vetor que represente aquele texto de alguma forma com n-dimensões para montar o hiperplano.\n",
    "\n",
    "  - Hiperparâmetros:\n",
    "    - kernel:\n",
    "    Kernel utilizado para o aumento da dimensionalidade do modelo. Usualmente para aplicações de NLP o linear costuma ser o melhor\n",
    "      \n",
    "    - C:\n",
    "    Parâmetro de regularização, \"afrouxa\" o critério de separação para ser possível separar mais facilmente os dados.\n",
    "\n",
    "- ### Gaussian Naive Bayes\n",
    "  O algoritmo Gausian Naive bayses consiste em fazer uma inferência baseado em várias curvas gaussianas adquiridas através das características do dataset de treino, onde cada uma delas é utilizada como uma parte para definir a probabilidade um dado ser de uma classe específica. No caso de um problema de NLP cada palavra possui sua curva gaussiana, associada com a probabilidade dela ser de uma classe ou outra. Em uma classificação binaria(positiva ou negativa), por exemplo, pode-se partir da pergunta: \"Esse texto é positivo?\" o algoritmo irá calcular a contagem de cada palavra presente no texto e repassar para cada curva gaussiana respectiva, no final irá tirar um score, a mesma coia será feita para a pergunta: \"Esse texto é negativo?\", calculando um novo score. Para a pergunta que obtiver o maior score será definida como a classe daquele novo input.\n",
    "  - Hiperparâmetros:\n",
    "    - var_smoothing:\n",
    "    Porção utilizada da maior variância, influencia diretamente na geração da curva.\n",
    "\n",
    "# Vetorizadores selecionados:  \n",
    "\n",
    "- ### CountVectorizer\n",
    "  Essa abordagem faz a contagem das palavras presente para cada uma das instâncias, no caso dessa aplicação tweets, as possibilidades são definidas baseadas no conjunto de todas as palavras possíveis de todos os tweets, o corpus. No final é gerado um vetor com a contagem de palavras presentes em cada tweet.\n",
    "\n",
    "- Hiperparâmetros:\n",
    "  - max_features:\n",
    "    Define a quantidade máxima de palavras que será mantido a contagem, no caso o algoritmo sempre priorizará as palavras que mais aparecem, pois, elas têm um maior peso para a definição da classe.\n",
    "  - analyzer:\n",
    "    Define se o algoritmo irá analisar palavra ou letras para a contagem.\n",
    "  - tokenizer:\n",
    "    Define o critério usado para separar as palavras no texto para serem contadas, dependendo da origem do texto pode melhorar muito a análise.\n",
    "    \n",
    "    \n",
    "- ### TfidfVectorizer\n",
    "  Essa abordagem faz a contagem das palavras por instância(tweets) assim como a CountVectorizer, porém além disso calcula a frequência que essa palavra apareceu baseado em todas as instâncias. Ou seja, uma palavra que aparece muito em um determinado tweet, mas muito pouco nos demais, terá um peso muito maior para a definição da classe daquele tweet. Do contrário, uma palavra que aparece em abundância,  em um tweet, mas é muito comum em todos os outros terá um peso menor.\n",
    "  \n",
    "- Hiperparâmetros:\n",
    "  - max_features:\n",
    "    Define a quantidade máxima de palavras que será mantido a contagem, no caso o algoritmo sempre priorizará as palavras que mais aparecem, pois, elas têm um maior peso para a definição da classe.\n",
    "  - analyzer:\n",
    "    Define se o algoritmo irá analisar palavra ou letras para a contagem.\n",
    "  - tokenizer:\n",
    "    Define o critério usado para separar as palavras no texto para serem contadas, dependendo da origem do texto pode melhorar muito a análise\n",
    "  \n",
    "\n",
    "# Redutores selecionado:\n",
    "\n",
    "- ### TruncatedSVD\n",
    "  Geralmente modelos que trabalham com NPL não lidam bem com vetores com uma grande quantidade de zeros seguidos, devido a numerosa quantidade de palavras possíveis dentro do corpus, as instâncias(tweets) não possuirão a maioria das palavras possíveis no corpus, gerando o problema citado acima. Para contornar isso é necessário reduzir para uma dimensão menor esses dados  que já foram filtrados anteriormente na contagem sendo os mais relevantes. Aplicando o redutor de dimensionalidade SVD, esse vetor espaçado com zeros será reduzido.\n",
    "\n",
    "- Hiperparâmetros:\n",
    "  - n_components: Define para quantas features o vetor será reduzido.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2e3ce8",
   "metadata": {},
   "source": [
    "# Treinamento\n",
    "A seguir é feito o treinamento do modelo propriamente dito. Cada etapa será combinada com as demais, formando todas as possibilidades possíveis do conjunto de dados carregado anteriormente. Para cada uma dessas combinações é carregado um conjunto de hiperpârametros associados a elas. Devido à abundância de dados será utilizado apenas uma validação cruzada dos hiperparâmetro apresentados, sendo aplicado uma separação  holdout nos dados de texto, garantindo uma maior confiabilidade na escolha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e4fb298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fiting best model to \n",
      "corpus_stemming_sem_stopwords__KNN__TfidfVectorizer__TrucatedSVD\n",
      "\n",
      "Fiting best model to \n",
      "corpus_stemming_sem_stopwords__KNN__CountVectorizer__TrucatedSVD\n",
      "\n",
      "Fiting best model to \n",
      "corpus_stemming_sem_stopwords__SMV__TfidfVectorizer__TrucatedSVD\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marvin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fiting best model to \n",
      "corpus_stemming_sem_stopwords__SMV__CountVectorizer__TrucatedSVD\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marvin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fiting best model to \n",
      "corpus_stemming_sem_stopwords__GaussianNB__TfidfVectorizer__TrucatedSVD\n",
      "\n",
      "Fiting best model to \n",
      "corpus_stemming_sem_stopwords__GaussianNB__CountVectorizer__TrucatedSVD\n",
      "\n",
      "Fiting best model to \n",
      "corpus_stemming_com_stopwords__KNN__TfidfVectorizer__TrucatedSVD\n",
      "\n",
      "Fiting best model to \n",
      "corpus_stemming_com_stopwords__KNN__CountVectorizer__TrucatedSVD\n",
      "\n",
      "Fiting best model to \n",
      "corpus_stemming_com_stopwords__SMV__TfidfVectorizer__TrucatedSVD\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marvin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fiting best model to \n",
      "corpus_stemming_com_stopwords__SMV__CountVectorizer__TrucatedSVD\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marvin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fiting best model to \n",
      "corpus_stemming_com_stopwords__GaussianNB__TfidfVectorizer__TrucatedSVD\n",
      "\n",
      "Fiting best model to \n",
      "corpus_stemming_com_stopwords__GaussianNB__CountVectorizer__TrucatedSVD\n",
      "\n",
      "Fiting best model to \n",
      "corpus_lematizado_sem_stopwords__KNN__TfidfVectorizer__TrucatedSVD\n",
      "\n",
      "Fiting best model to \n",
      "corpus_lematizado_sem_stopwords__KNN__CountVectorizer__TrucatedSVD\n",
      "\n",
      "Fiting best model to \n",
      "corpus_lematizado_sem_stopwords__SMV__TfidfVectorizer__TrucatedSVD\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marvin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fiting best model to \n",
      "corpus_lematizado_sem_stopwords__SMV__CountVectorizer__TrucatedSVD\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marvin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fiting best model to \n",
      "corpus_lematizado_sem_stopwords__GaussianNB__TfidfVectorizer__TrucatedSVD\n",
      "\n",
      "Fiting best model to \n",
      "corpus_lematizado_sem_stopwords__GaussianNB__CountVectorizer__TrucatedSVD\n",
      "\n",
      "Fiting best model to \n",
      "corpus_lematizado_com_stopwords__KNN__TfidfVectorizer__TrucatedSVD\n",
      "\n",
      "Fiting best model to \n",
      "corpus_lematizado_com_stopwords__KNN__CountVectorizer__TrucatedSVD\n",
      "\n",
      "Fiting best model to \n",
      "corpus_lematizado_com_stopwords__SMV__TfidfVectorizer__TrucatedSVD\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marvin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fiting best model to \n",
      "corpus_lematizado_com_stopwords__SMV__CountVectorizer__TrucatedSVD\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marvin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fiting best model to \n",
      "corpus_lematizado_com_stopwords__GaussianNB__TfidfVectorizer__TrucatedSVD\n",
      "\n",
      "Fiting best model to \n",
      "corpus_lematizado_com_stopwords__GaussianNB__CountVectorizer__TrucatedSVD\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_splits_cv = 1\n",
    "n_splits_gs = 5\n",
    "\n",
    "all_scores = {}\n",
    "\n",
    "split_cv = ShuffleSplit(n_splits=n_splits_cv, test_size=0.2, random_state=42)\n",
    "\n",
    "for corpus_name, corpus_data in corpus.items():\n",
    "\n",
    "    for model_name, model_data in models.items():\n",
    "\n",
    "        model_params = {\n",
    "            f\"model__{key}\": value\n",
    "            for key, value in model_data[\"hyperparameters\"].items()\n",
    "        }\n",
    "\n",
    "        for vectorizer_name, vectorizer_data in vectorizers.items():\n",
    "\n",
    "            vectorize_params = {\n",
    "                f\"vectorizer__{key}\": value\n",
    "                for key, value in vectorizer_data[\"hyperparameters\"].items()\n",
    "            }\n",
    "\n",
    "            for reducer_name, reducer_data in reducer.items():\n",
    "\n",
    "                reducer_params = {\n",
    "                    f\"reducer__{key}\": value\n",
    "                    for key, value in reducer_data[\"hyperparameters\"].items()\n",
    "                }\n",
    "\n",
    "                param_distributions = {\n",
    "                    **model_params,\n",
    "                    **vectorize_params,\n",
    "                    **reducer_params,\n",
    "                }\n",
    "\n",
    "                pipeline = Pipeline(\n",
    "                    steps=[\n",
    "                        (\"vectorizer\", vectorizer_data[\"vectorizer_obj\"]),\n",
    "                        (\"reducer\", reducer_data[\"reducer_obj\"]),\n",
    "                        (\"scaler\", StandardScaler()),\n",
    "                        (\"model\", model_data[\"model_obj\"]),\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "                approach_name = (\n",
    "                    f\"{corpus_name}__{model_name}__{vectorizer_name}__{reducer_name}\"\n",
    "                )\n",
    "\n",
    "                print(f\"Fiting best model to \\n{approach_name}\", end=\"\\n\\n\")\n",
    "\n",
    "                tuned_pipeline = RandomizedSearchCV(\n",
    "                    pipeline,\n",
    "                    param_distributions,\n",
    "                    scoring=\"f1\",\n",
    "                    cv=n_splits_gs,\n",
    "                    random_state=42,\n",
    "                    n_jobs=6,\n",
    "                )\n",
    "\n",
    "                scores = cross_validate(\n",
    "                    tuned_pipeline,\n",
    "                    corpus_data[\"corpus_data\"],\n",
    "                    corpus_data[\"corpus_labels\"],\n",
    "                    cv=split_cv,\n",
    "                    scoring=[\"accuracy\", \"f1\", \"recall\"],\n",
    "                )\n",
    "\n",
    "                all_scores.update(\n",
    "                    {\n",
    "                        approach_name: {\n",
    "                            \"scores\": scores,\n",
    "                        }\n",
    "                    }\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb70f9c",
   "metadata": {},
   "source": [
    "# Criação de um dataframe com todos os modelos criados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f7511e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_20aa1_row0_col1, #T_20aa1_row23_col4 {\n",
       "  background-color: #fbf4f9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_20aa1_row0_col2, #T_20aa1_row3_col3 {\n",
       "  background-color: #034973;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_20aa1_row0_col3 {\n",
       "  background-color: #d9d8ea;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_20aa1_row0_col4 {\n",
       "  background-color: #2484ba;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_20aa1_row0_col5, #T_20aa1_row1_col2, #T_20aa1_row2_col3, #T_20aa1_row2_col4, #T_20aa1_row9_col1, #T_20aa1_row21_col1 {\n",
       "  background-color: #023858;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_20aa1_row1_col1, #T_20aa1_row6_col1, #T_20aa1_row7_col1, #T_20aa1_row12_col1, #T_20aa1_row13_col1, #T_20aa1_row18_col1, #T_20aa1_row19_col1 {\n",
       "  background-color: #fcf4fa;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_20aa1_row1_col3 {\n",
       "  background-color: #8cb3d5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_20aa1_row1_col4, #T_20aa1_row1_col5 {\n",
       "  background-color: #63a2cb;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_20aa1_row2_col1, #T_20aa1_row3_col1, #T_20aa1_row14_col4 {\n",
       "  background-color: #023d60;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_20aa1_row2_col2, #T_20aa1_row3_col2, #T_20aa1_row4_col2, #T_20aa1_row5_col2 {\n",
       "  background-color: #faf3f9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_20aa1_row2_col5, #T_20aa1_row3_col4, #T_20aa1_row10_col5 {\n",
       "  background-color: #045280;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_20aa1_row3_col5, #T_20aa1_row19_col5 {\n",
       "  background-color: #056dac;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_20aa1_row4_col1, #T_20aa1_row5_col1, #T_20aa1_row5_col4, #T_20aa1_row5_col5, #T_20aa1_row8_col2, #T_20aa1_row9_col2, #T_20aa1_row10_col1, #T_20aa1_row11_col1, #T_20aa1_row11_col2, #T_20aa1_row16_col1, #T_20aa1_row17_col1, #T_20aa1_row22_col1, #T_20aa1_row23_col1, #T_20aa1_row23_col3 {\n",
       "  background-color: #fff7fb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_20aa1_row4_col3 {\n",
       "  background-color: #d5d5e8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_20aa1_row4_col4 {\n",
       "  background-color: #529bc7;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_20aa1_row4_col5 {\n",
       "  background-color: #056ba9;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_20aa1_row5_col3 {\n",
       "  background-color: #f5eef6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_20aa1_row6_col2, #T_20aa1_row7_col2, #T_20aa1_row13_col2 {\n",
       "  background-color: #045d92;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_20aa1_row6_col3 {\n",
       "  background-color: #e0dded;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_20aa1_row6_col4, #T_20aa1_row10_col4 {\n",
       "  background-color: #2d8abd;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_20aa1_row6_col5 {\n",
       "  background-color: #023e62;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_20aa1_row7_col3 {\n",
       "  background-color: #e8e4f0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_20aa1_row7_col4 {\n",
       "  background-color: #6da6cd;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_20aa1_row7_col5 {\n",
       "  background-color: #0872b1;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_20aa1_row8_col1, #T_20aa1_row14_col1, #T_20aa1_row20_col1 {\n",
       "  background-color: #023f64;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_20aa1_row8_col3 {\n",
       "  background-color: #045585;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_20aa1_row8_col4, #T_20aa1_row12_col2 {\n",
       "  background-color: #034369;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_20aa1_row8_col5 {\n",
       "  background-color: #034871;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_20aa1_row9_col3 {\n",
       "  background-color: #04639b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_20aa1_row9_col4 {\n",
       "  background-color: #04649e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_20aa1_row9_col5 {\n",
       "  background-color: #197db7;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_20aa1_row10_col2, #T_20aa1_row20_col2, #T_20aa1_row21_col2, #T_20aa1_row23_col2 {\n",
       "  background-color: #fef6fb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_20aa1_row10_col3 {\n",
       "  background-color: #ced0e6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_20aa1_row11_col3 {\n",
       "  background-color: #efe9f3;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_20aa1_row11_col4 {\n",
       "  background-color: #f0eaf4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_20aa1_row11_col5 {\n",
       "  background-color: #eee9f3;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_20aa1_row12_col3 {\n",
       "  background-color: #abbfdc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_20aa1_row12_col4 {\n",
       "  background-color: #5ea0ca;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_20aa1_row12_col5 {\n",
       "  background-color: #3b92c1;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_20aa1_row13_col3 {\n",
       "  background-color: #e1dfed;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_20aa1_row13_col4 {\n",
       "  background-color: #7bacd1;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_20aa1_row13_col5 {\n",
       "  background-color: #2786bb;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_20aa1_row14_col2 {\n",
       "  background-color: #f8f1f8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_20aa1_row14_col3 {\n",
       "  background-color: #02395a;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_20aa1_row14_col5 {\n",
       "  background-color: #045b8f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_20aa1_row15_col1 {\n",
       "  background-color: #023a5b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_20aa1_row15_col2, #T_20aa1_row16_col2, #T_20aa1_row17_col2 {\n",
       "  background-color: #f9f2f8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_20aa1_row15_col3 {\n",
       "  background-color: #03466e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_20aa1_row15_col4 {\n",
       "  background-color: #045687;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_20aa1_row15_col5 {\n",
       "  background-color: #1077b4;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_20aa1_row16_col3, #T_20aa1_row17_col5 {\n",
       "  background-color: #ece7f2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_20aa1_row16_col4 {\n",
       "  background-color: #4897c4;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_20aa1_row16_col5 {\n",
       "  background-color: #045382;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_20aa1_row17_col3 {\n",
       "  background-color: #dfddec;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_20aa1_row17_col4 {\n",
       "  background-color: #e6e2ef;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_20aa1_row18_col2 {\n",
       "  background-color: #046096;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_20aa1_row18_col3 {\n",
       "  background-color: #ede8f3;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_20aa1_row18_col4 {\n",
       "  background-color: #4496c3;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_20aa1_row18_col5 {\n",
       "  background-color: #034c78;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_20aa1_row19_col2 {\n",
       "  background-color: #045a8d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_20aa1_row19_col3 {\n",
       "  background-color: #f5eff6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_20aa1_row19_col4 {\n",
       "  background-color: #78abd0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_20aa1_row20_col3 {\n",
       "  background-color: #0568a3;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_20aa1_row20_col4 {\n",
       "  background-color: #03517e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_20aa1_row20_col5 {\n",
       "  background-color: #034a74;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_20aa1_row21_col3 {\n",
       "  background-color: #187cb6;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_20aa1_row21_col4 {\n",
       "  background-color: #1278b4;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_20aa1_row21_col5 {\n",
       "  background-color: #358fc0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_20aa1_row22_col2 {\n",
       "  background-color: #fef6fa;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_20aa1_row22_col3 {\n",
       "  background-color: #c1cae2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_20aa1_row22_col4 {\n",
       "  background-color: #2c89bd;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_20aa1_row22_col5 {\n",
       "  background-color: #04588a;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_20aa1_row23_col5 {\n",
       "  background-color: #f4eef6;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_20aa1\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_20aa1_level0_col0\" class=\"col_heading level0 col0\" >approach_name</th>\n",
       "      <th id=\"T_20aa1_level0_col1\" class=\"col_heading level0 col1\" >fit_time</th>\n",
       "      <th id=\"T_20aa1_level0_col2\" class=\"col_heading level0 col2\" >score_time</th>\n",
       "      <th id=\"T_20aa1_level0_col3\" class=\"col_heading level0 col3\" >accuracy</th>\n",
       "      <th id=\"T_20aa1_level0_col4\" class=\"col_heading level0 col4\" >f1</th>\n",
       "      <th id=\"T_20aa1_level0_col5\" class=\"col_heading level0 col5\" >recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_20aa1_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_20aa1_row0_col0\" class=\"data row0 col0\" >corpus_stemming_sem_stopwords__KNN__TfidfVectorizer__TrucatedSVD</td>\n",
       "      <td id=\"T_20aa1_row0_col1\" class=\"data row0 col1\" >199.141841</td>\n",
       "      <td id=\"T_20aa1_row0_col2\" class=\"data row0 col2\" >11.772794</td>\n",
       "      <td id=\"T_20aa1_row0_col3\" class=\"data row0 col3\" >0.654994</td>\n",
       "      <td id=\"T_20aa1_row0_col4\" class=\"data row0 col4\" >0.692497</td>\n",
       "      <td id=\"T_20aa1_row0_col5\" class=\"data row0 col5\" >0.779778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_20aa1_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_20aa1_row1_col0\" class=\"data row1 col0\" >corpus_stemming_sem_stopwords__KNN__CountVectorizer__TrucatedSVD</td>\n",
       "      <td id=\"T_20aa1_row1_col1\" class=\"data row1 col1\" >181.735415</td>\n",
       "      <td id=\"T_20aa1_row1_col2\" class=\"data row1 col2\" >12.541844</td>\n",
       "      <td id=\"T_20aa1_row1_col3\" class=\"data row1 col3\" >0.679662</td>\n",
       "      <td id=\"T_20aa1_row1_col4\" class=\"data row1 col4\" >0.671734</td>\n",
       "      <td id=\"T_20aa1_row1_col5\" class=\"data row1 col5\" >0.657895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_20aa1_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_20aa1_row2_col0\" class=\"data row2 col0\" >corpus_stemming_sem_stopwords__SMV__TfidfVectorizer__TrucatedSVD</td>\n",
       "      <td id=\"T_20aa1_row2_col1\" class=\"data row2 col1\" >5564.609405</td>\n",
       "      <td id=\"T_20aa1_row2_col2\" class=\"data row2 col2\" >1.235289</td>\n",
       "      <td id=\"T_20aa1_row2_col3\" class=\"data row2 col3\" >0.739607</td>\n",
       "      <td id=\"T_20aa1_row2_col4\" class=\"data row2 col4\" >0.742911</td>\n",
       "      <td id=\"T_20aa1_row2_col5\" class=\"data row2 col5\" >0.755194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_20aa1_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_20aa1_row3_col0\" class=\"data row3 col0\" >corpus_stemming_sem_stopwords__SMV__CountVectorizer__TrucatedSVD</td>\n",
       "      <td id=\"T_20aa1_row3_col1\" class=\"data row3 col1\" >5572.788386</td>\n",
       "      <td id=\"T_20aa1_row3_col2\" class=\"data row3 col2\" >1.229288</td>\n",
       "      <td id=\"T_20aa1_row3_col3\" class=\"data row3 col3\" >0.732879</td>\n",
       "      <td id=\"T_20aa1_row3_col4\" class=\"data row3 col4\" >0.728262</td>\n",
       "      <td id=\"T_20aa1_row3_col5\" class=\"data row3 col5\" >0.718490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_20aa1_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_20aa1_row4_col0\" class=\"data row4 col0\" >corpus_stemming_sem_stopwords__GaussianNB__TfidfVectorizer__TrucatedSVD</td>\n",
       "      <td id=\"T_20aa1_row4_col1\" class=\"data row4 col1\" >62.276789</td>\n",
       "      <td id=\"T_20aa1_row4_col2\" class=\"data row4 col2\" >1.250131</td>\n",
       "      <td id=\"T_20aa1_row4_col3\" class=\"data row4 col3\" >0.657150</td>\n",
       "      <td id=\"T_20aa1_row4_col4\" class=\"data row4 col4\" >0.677118</td>\n",
       "      <td id=\"T_20aa1_row4_col5\" class=\"data row4 col5\" >0.721607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_20aa1_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_20aa1_row5_col0\" class=\"data row5 col0\" >corpus_stemming_sem_stopwords__GaussianNB__CountVectorizer__TrucatedSVD</td>\n",
       "      <td id=\"T_20aa1_row5_col1\" class=\"data row5 col1\" >59.803931</td>\n",
       "      <td id=\"T_20aa1_row5_col2\" class=\"data row5 col2\" >1.251293</td>\n",
       "      <td id=\"T_20aa1_row5_col3\" class=\"data row5 col3\" >0.640245</td>\n",
       "      <td id=\"T_20aa1_row5_col4\" class=\"data row5 col4\" >0.590074</td>\n",
       "      <td id=\"T_20aa1_row5_col5\" class=\"data row5 col5\" >0.519737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_20aa1_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_20aa1_row6_col0\" class=\"data row6 col0\" >corpus_stemming_com_stopwords__KNN__TfidfVectorizer__TrucatedSVD</td>\n",
       "      <td id=\"T_20aa1_row6_col1\" class=\"data row6 col1\" >169.162868</td>\n",
       "      <td id=\"T_20aa1_row6_col2\" class=\"data row6 col2\" >10.868898</td>\n",
       "      <td id=\"T_20aa1_row6_col3\" class=\"data row6 col3\" >0.651975</td>\n",
       "      <td id=\"T_20aa1_row6_col4\" class=\"data row6 col4\" >0.688777</td>\n",
       "      <td id=\"T_20aa1_row6_col5\" class=\"data row6 col5\" >0.773026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_20aa1_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_20aa1_row7_col0\" class=\"data row7 col0\" >corpus_stemming_com_stopwords__KNN__CountVectorizer__TrucatedSVD</td>\n",
       "      <td id=\"T_20aa1_row7_col1\" class=\"data row7 col1\" >167.415187</td>\n",
       "      <td id=\"T_20aa1_row7_col2\" class=\"data row7 col2\" >10.870465</td>\n",
       "      <td id=\"T_20aa1_row7_col3\" class=\"data row7 col3\" >0.648094</td>\n",
       "      <td id=\"T_20aa1_row7_col4\" class=\"data row7 col4\" >0.668562</td>\n",
       "      <td id=\"T_20aa1_row7_col5\" class=\"data row7 col5\" >0.712431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_20aa1_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_20aa1_row8_col0\" class=\"data row8 col0\" >corpus_stemming_com_stopwords__SMV__TfidfVectorizer__TrucatedSVD</td>\n",
       "      <td id=\"T_20aa1_row8_col1\" class=\"data row8 col1\" >5524.359979</td>\n",
       "      <td id=\"T_20aa1_row8_col2\" class=\"data row8 col2\" >0.842190</td>\n",
       "      <td id=\"T_20aa1_row8_col3\" class=\"data row8 col3\" >0.727963</td>\n",
       "      <td id=\"T_20aa1_row8_col4\" class=\"data row8 col4\" >0.736684</td>\n",
       "      <td id=\"T_20aa1_row8_col5\" class=\"data row8 col5\" >0.763850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_20aa1_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_20aa1_row9_col0\" class=\"data row9 col0\" >corpus_stemming_com_stopwords__SMV__CountVectorizer__TrucatedSVD</td>\n",
       "      <td id=\"T_20aa1_row9_col1\" class=\"data row9 col1\" >5681.801791</td>\n",
       "      <td id=\"T_20aa1_row9_col2\" class=\"data row9 col2\" >0.840954</td>\n",
       "      <td id=\"T_20aa1_row9_col3\" class=\"data row9 col3\" >0.720804</td>\n",
       "      <td id=\"T_20aa1_row9_col4\" class=\"data row9 col4\" >0.714475</td>\n",
       "      <td id=\"T_20aa1_row9_col5\" class=\"data row9 col5\" >0.701177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_20aa1_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_20aa1_row10_col0\" class=\"data row10 col0\" >corpus_stemming_com_stopwords__GaussianNB__TfidfVectorizer__TrucatedSVD</td>\n",
       "      <td id=\"T_20aa1_row10_col1\" class=\"data row10 col1\" >56.793762</td>\n",
       "      <td id=\"T_20aa1_row10_col2\" class=\"data row10 col2\" >0.899981</td>\n",
       "      <td id=\"T_20aa1_row10_col3\" class=\"data row10 col3\" >0.660255</td>\n",
       "      <td id=\"T_20aa1_row10_col4\" class=\"data row10 col4\" >0.688887</td>\n",
       "      <td id=\"T_20aa1_row10_col5\" class=\"data row10 col5\" >0.755021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_20aa1_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_20aa1_row11_col0\" class=\"data row11 col0\" >corpus_stemming_com_stopwords__GaussianNB__CountVectorizer__TrucatedSVD</td>\n",
       "      <td id=\"T_20aa1_row11_col1\" class=\"data row11 col1\" >54.630966</td>\n",
       "      <td id=\"T_20aa1_row11_col2\" class=\"data row11 col2\" >0.857194</td>\n",
       "      <td id=\"T_20aa1_row11_col3\" class=\"data row11 col3\" >0.644385</td>\n",
       "      <td id=\"T_20aa1_row11_col4\" class=\"data row11 col4\" >0.605794</td>\n",
       "      <td id=\"T_20aa1_row11_col5\" class=\"data row11 col5\" >0.548476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_20aa1_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_20aa1_row12_col0\" class=\"data row12 col0\" >corpus_lematizado_sem_stopwords__KNN__TfidfVectorizer__TrucatedSVD</td>\n",
       "      <td id=\"T_20aa1_row12_col1\" class=\"data row12 col1\" >179.238782</td>\n",
       "      <td id=\"T_20aa1_row12_col2\" class=\"data row12 col2\" >12.043730</td>\n",
       "      <td id=\"T_20aa1_row12_col3\" class=\"data row12 col3\" >0.671209</td>\n",
       "      <td id=\"T_20aa1_row12_col4\" class=\"data row12 col4\" >0.673238</td>\n",
       "      <td id=\"T_20aa1_row12_col5\" class=\"data row12 col5\" >0.679882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_20aa1_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_20aa1_row13_col0\" class=\"data row13 col0\" >corpus_lematizado_sem_stopwords__KNN__CountVectorizer__TrucatedSVD</td>\n",
       "      <td id=\"T_20aa1_row13_col1\" class=\"data row13 col1\" >169.527628</td>\n",
       "      <td id=\"T_20aa1_row13_col2\" class=\"data row13 col2\" >10.880467</td>\n",
       "      <td id=\"T_20aa1_row13_col3\" class=\"data row13 col3\" >0.651113</td>\n",
       "      <td id=\"T_20aa1_row13_col4\" class=\"data row13 col4\" >0.663897</td>\n",
       "      <td id=\"T_20aa1_row13_col5\" class=\"data row13 col5\" >0.691655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_20aa1_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_20aa1_row14_col0\" class=\"data row14 col0\" >corpus_lematizado_sem_stopwords__SMV__TfidfVectorizer__TrucatedSVD</td>\n",
       "      <td id=\"T_20aa1_row14_col1\" class=\"data row14 col1\" >5514.193325</td>\n",
       "      <td id=\"T_20aa1_row14_col2\" class=\"data row14 col2\" >1.346303</td>\n",
       "      <td id=\"T_20aa1_row14_col3\" class=\"data row14 col3\" >0.739003</td>\n",
       "      <td id=\"T_20aa1_row14_col4\" class=\"data row14 col4\" >0.739766</td>\n",
       "      <td id=\"T_20aa1_row14_col5\" class=\"data row14 col5\" >0.744633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_20aa1_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "      <td id=\"T_20aa1_row15_col0\" class=\"data row15 col0\" >corpus_lematizado_sem_stopwords__SMV__CountVectorizer__TrucatedSVD</td>\n",
       "      <td id=\"T_20aa1_row15_col1\" class=\"data row15 col1\" >5634.616037</td>\n",
       "      <td id=\"T_20aa1_row15_col2\" class=\"data row15 col2\" >1.317288</td>\n",
       "      <td id=\"T_20aa1_row15_col3\" class=\"data row15 col3\" >0.734000</td>\n",
       "      <td id=\"T_20aa1_row15_col4\" class=\"data row15 col4\" >0.725867</td>\n",
       "      <td id=\"T_20aa1_row15_col5\" class=\"data row15 col5\" >0.706891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_20aa1_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "      <td id=\"T_20aa1_row16_col0\" class=\"data row16 col0\" >corpus_lematizado_sem_stopwords__GaussianNB__TfidfVectorizer__TrucatedSVD</td>\n",
       "      <td id=\"T_20aa1_row16_col1\" class=\"data row16 col1\" >62.743404</td>\n",
       "      <td id=\"T_20aa1_row16_col2\" class=\"data row16 col2\" >1.334176</td>\n",
       "      <td id=\"T_20aa1_row16_col3\" class=\"data row16 col3\" >0.646110</td>\n",
       "      <td id=\"T_20aa1_row16_col4\" class=\"data row16 col4\" >0.679878</td>\n",
       "      <td id=\"T_20aa1_row16_col5\" class=\"data row16 col5\" >0.754328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_20aa1_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "      <td id=\"T_20aa1_row17_col0\" class=\"data row17 col0\" >corpus_lematizado_sem_stopwords__GaussianNB__CountVectorizer__TrucatedSVD</td>\n",
       "      <td id=\"T_20aa1_row17_col1\" class=\"data row17 col1\" >61.380245</td>\n",
       "      <td id=\"T_20aa1_row17_col2\" class=\"data row17 col2\" >1.339303</td>\n",
       "      <td id=\"T_20aa1_row17_col3\" class=\"data row17 col3\" >0.652751</td>\n",
       "      <td id=\"T_20aa1_row17_col4\" class=\"data row17 col4\" >0.613479</td>\n",
       "      <td id=\"T_20aa1_row17_col5\" class=\"data row17 col5\" >0.553151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_20aa1_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "      <td id=\"T_20aa1_row18_col0\" class=\"data row18 col0\" >corpus_lematizado_com_stopwords__KNN__TfidfVectorizer__TrucatedSVD</td>\n",
       "      <td id=\"T_20aa1_row18_col1\" class=\"data row18 col1\" >168.159230</td>\n",
       "      <td id=\"T_20aa1_row18_col2\" class=\"data row18 col2\" >10.707427</td>\n",
       "      <td id=\"T_20aa1_row18_col3\" class=\"data row18 col3\" >0.645420</td>\n",
       "      <td id=\"T_20aa1_row18_col4\" class=\"data row18 col4\" >0.681194</td>\n",
       "      <td id=\"T_20aa1_row18_col5\" class=\"data row18 col5\" >0.760388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_20aa1_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
       "      <td id=\"T_20aa1_row19_col0\" class=\"data row19 col0\" >corpus_lematizado_com_stopwords__KNN__CountVectorizer__TrucatedSVD</td>\n",
       "      <td id=\"T_20aa1_row19_col1\" class=\"data row19 col1\" >166.931856</td>\n",
       "      <td id=\"T_20aa1_row19_col2\" class=\"data row19 col2\" >11.064508</td>\n",
       "      <td id=\"T_20aa1_row19_col3\" class=\"data row19 col3\" >0.639641</td>\n",
       "      <td id=\"T_20aa1_row19_col4\" class=\"data row19 col4\" >0.665117</td>\n",
       "      <td id=\"T_20aa1_row19_col5\" class=\"data row19 col5\" >0.718317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_20aa1_level0_row20\" class=\"row_heading level0 row20\" >20</th>\n",
       "      <td id=\"T_20aa1_row20_col0\" class=\"data row20 col0\" >corpus_lematizado_com_stopwords__SMV__TfidfVectorizer__TrucatedSVD</td>\n",
       "      <td id=\"T_20aa1_row20_col1\" class=\"data row20 col1\" >5518.484990</td>\n",
       "      <td id=\"T_20aa1_row20_col2\" class=\"data row20 col2\" >0.904205</td>\n",
       "      <td id=\"T_20aa1_row20_col3\" class=\"data row20 col3\" >0.717699</td>\n",
       "      <td id=\"T_20aa1_row20_col4\" class=\"data row20 col4\" >0.729034</td>\n",
       "      <td id=\"T_20aa1_row20_col5\" class=\"data row20 col5\" >0.762292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_20aa1_level0_row21\" class=\"row_heading level0 row21\" >21</th>\n",
       "      <td id=\"T_20aa1_row21_col0\" class=\"data row21 col0\" >corpus_lematizado_com_stopwords__SMV__CountVectorizer__TrucatedSVD</td>\n",
       "      <td id=\"T_20aa1_row21_col1\" class=\"data row21 col1\" >5683.137896</td>\n",
       "      <td id=\"T_20aa1_row21_col2\" class=\"data row21 col2\" >0.907215</td>\n",
       "      <td id=\"T_20aa1_row21_col3\" class=\"data row21 col3\" >0.707780</td>\n",
       "      <td id=\"T_20aa1_row21_col4\" class=\"data row21 col4\" >0.699539</td>\n",
       "      <td id=\"T_20aa1_row21_col5\" class=\"data row21 col5\" >0.682825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_20aa1_level0_row22\" class=\"row_heading level0 row22\" >22</th>\n",
       "      <td id=\"T_20aa1_row22_col0\" class=\"data row22 col0\" >corpus_lematizado_com_stopwords__GaussianNB__TfidfVectorizer__TrucatedSVD</td>\n",
       "      <td id=\"T_20aa1_row22_col1\" class=\"data row22 col1\" >64.933789</td>\n",
       "      <td id=\"T_20aa1_row22_col2\" class=\"data row22 col2\" >0.945214</td>\n",
       "      <td id=\"T_20aa1_row22_col3\" class=\"data row22 col3\" >0.664050</td>\n",
       "      <td id=\"T_20aa1_row22_col4\" class=\"data row22 col4\" >0.689567</td>\n",
       "      <td id=\"T_20aa1_row22_col5\" class=\"data row22 col5\" >0.748961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_20aa1_level0_row23\" class=\"row_heading level0 row23\" >23</th>\n",
       "      <td id=\"T_20aa1_row23_col0\" class=\"data row23 col0\" >corpus_lematizado_com_stopwords__GaussianNB__CountVectorizer__TrucatedSVD</td>\n",
       "      <td id=\"T_20aa1_row23_col1\" class=\"data row23 col1\" >58.685468</td>\n",
       "      <td id=\"T_20aa1_row23_col2\" class=\"data row23 col2\" >0.915218</td>\n",
       "      <td id=\"T_20aa1_row23_col3\" class=\"data row23 col3\" >0.632741</td>\n",
       "      <td id=\"T_20aa1_row23_col4\" class=\"data row23 col4\" >0.593780</td>\n",
       "      <td id=\"T_20aa1_row23_col5\" class=\"data row23 col5\" >0.538781</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x179e47d7e80>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    approach_names,\n",
    "    fit_times,\n",
    "    scores_times,\n",
    "    accuracy_means,\n",
    "    f1_scores_mean,\n",
    "    recall_scores_mean,\n",
    ") = ([], [], [], [], [], [])\n",
    "\n",
    "\n",
    "for approach_name, score in all_scores.items():\n",
    "    approach_names.append(approach_name)\n",
    "    fit_times.append(score[\"scores\"][\"fit_time\"].mean())\n",
    "    scores_times.append(score[\"scores\"][\"score_time\"].mean())\n",
    "    accuracy_means.append(score[\"scores\"][\"test_accuracy\"].mean())\n",
    "    f1_scores_mean.append(score[\"scores\"][\"test_f1\"].mean())\n",
    "    recall_scores_mean.append(score[\"scores\"][\"test_recall\"].mean())\n",
    "\n",
    "test_data = data = {\n",
    "    \"approach_name\": approach_names,\n",
    "    \"fit_time\": fit_times,\n",
    "    \"score_time\": scores_times,\n",
    "    \"accuracy\": accuracy_means,\n",
    "    \"f1\": f1_scores_mean,\n",
    "    \"recall\": recall_scores_mean,\n",
    "}\n",
    "\n",
    "\n",
    "test_data_df = pd.DataFrame(test_data)\n",
    "\n",
    "\n",
    "test_data_df.style.background_gradient()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553a71dc",
   "metadata": {},
   "source": [
    "# Definição do melhor modelo, será utilizado a moda entre as métricas de f1, recall e accuracy fit_time e score_time, sendo essas duas ultimas menor melhor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96f393ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "approach_names = test_data_df.approach_name\n",
    "\n",
    "\n",
    "def get_best_model(x):\n",
    "    if x.name.endswith(\"time\"):\n",
    "        return approach_names[np.argmin(x.values)]\n",
    "\n",
    "    return approach_names[np.argmax(x.values)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56dffedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best approach is \n",
      "{'corpus': 'corpus_stemming_sem_stopwords', 'model': 'SMV', 'vectorizer': 'TfidfVectorizer', 'reducer': 'TrucatedSVD'}\n"
     ]
    }
   ],
   "source": [
    "best_approach_name = mode(test_data_df.apply(get_best_model, axis=0).to_list()).split(\n",
    "    \"__\"\n",
    ")\n",
    "names = [\"corpus\", \"model\", \"vectorizer\", \"reducer\"]\n",
    "\n",
    "best_approach_dict = {name: value for name,\n",
    "                      value in zip(names, best_approach_name)}\n",
    "\n",
    "print(f\"Best approach is \\n{best_approach_dict}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48933ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_reducer_hyperparameters = reducer[best_approach_dict[\"reducer\"]\n",
    "                                       ][\"hyperparameters\"]\n",
    "best_vectorizer_hyperparameters = vectorizers[best_approach_dict[\"vectorizer\"]][\n",
    "    \"hyperparameters\"\n",
    "]\n",
    "best_model_hyperparameters = models[best_approach_dict[\"model\"]\n",
    "                                    ][\"hyperparameters\"]\n",
    "\n",
    "\n",
    "reducer_params = {\n",
    "    f\"reducer__{key}\": value for key, value in best_reducer_hyperparameters.items()\n",
    "}\n",
    "vectorize_params = {\n",
    "    f\"vectorizer__{key}\": value\n",
    "    for key, value in best_vectorizer_hyperparameters.items()\n",
    "}\n",
    "model_params = {\n",
    "    f\"model__{key}\": value for key, value in best_model_hyperparameters.items()\n",
    "}\n",
    "\n",
    "\n",
    "param_distributions = {\n",
    "    **model_params,\n",
    "    **vectorize_params,\n",
    "    **reducer_params,\n",
    "}\n",
    "\n",
    "\n",
    "best_obj_vectorizer = vectorizers[best_approach_dict[\"vectorizer\"]\n",
    "                                  ][\"vectorizer_obj\"]\n",
    "\n",
    "best_obj_reducer = reducer[best_approach_dict[\"reducer\"]][\"reducer_obj\"]\n",
    "\n",
    "best_obj_model = models[best_approach_dict[\"model\"]][\"model_obj\"]\n",
    "\n",
    "corpus_final = corpus[best_approach_dict[\"corpus\"]][\"corpus_data\"]\n",
    "labels_final = corpus[best_approach_dict[\"corpus\"]][\"corpus_labels\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19d58b5",
   "metadata": {},
   "source": [
    "# Após a definição da melhor abordagem é necessário fazer novamente a validação cruzada dos hiperparâmetros, mas agora com todos os dados disponíveis, para garantir uma generalização maior do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5b68224",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_n_splits_final = 5\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\n",
    "            \"vectorizer\",\n",
    "            best_obj_vectorizer,\n",
    "        ),\n",
    "        (\n",
    "            \"reducer\",\n",
    "            best_obj_reducer,\n",
    "        ),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"model\", best_obj_model),\n",
    "    ]\n",
    ")\n",
    "tuned_pipeline = RandomizedSearchCV(\n",
    "    pipeline,\n",
    "    param_distributions,\n",
    "    scoring=\"f1\",\n",
    "    cv=cv_n_splits_final,\n",
    "    random_state=42,\n",
    "    n_jobs=6,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ab380d",
   "metadata": {},
   "source": [
    "# Finalizado tudo o melhor modelo é ajustado e persistido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "440a3872",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marvin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Marvin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../models/best_modelv2.joblib']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuned_pipeline.fit(corpus_final, labels_final)\n",
    "best_estimator = tuned_pipeline.best_estimator_\n",
    "best_estimator.fit(corpus_final, labels_final)\n",
    "\n",
    "model_path = \"../models/best_modelv2.joblib\"\n",
    "dump(best_estimator, model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4865dbc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[21164,  7795],\n",
       "       [ 6812, 22195]], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = best_estimator.predict(corpus_final)\n",
    "confusion_matrix(labels_final, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652eecdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079e22d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5b79d64c7260b6a774f38d217b59fd5aa6cc70b038ea6cc711b365a8daa819f7"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "882c8c6501720d7339a894262ca6c76e47c685bc126574a2b14e77f6df77b0f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
