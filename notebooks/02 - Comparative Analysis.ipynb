{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a9d945a-b822-4070-a5b7-e337c83c850c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/marvin-\n",
      "[nltk_data]     linux/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import (\n",
    "    CountVectorizer,\n",
    "    TfidfVectorizer,\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    ShuffleSplit,\n",
    "    cross_validate,\n",
    "    RandomizedSearchCV,\n",
    ")\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score\n",
    "from sklearn import metrics\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "nltk.download(\"stopwords\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5510de",
   "metadata": {},
   "source": [
    "### Carregar o dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79285eca-d73c-4258-a24c-93101f1bcee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    \"../data/raw/NoThemeTweets.csv\", usecols=[\"tweet_text\", \"sentiment\"]\n",
    ").sample(9000, random_state=42)\n",
    "\n",
    "# x_train, x_test, y_train, y_test = train_test_split(df.tweet_text,df.sentiment,test_size=0.2, random_state=42)\n",
    "\n",
    "# len(x_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45f3f48c-c8fa-4c21-ba89-161b5992cb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus_train = df.tweet_text[2000:].to_list()\n",
    "# labels_train = df.sentiment[2000:].replace({\"Positivo\": 1, \"Negativo\": 0}).to_list()\n",
    "\n",
    "corpus_train = df.tweet_text.to_list()\n",
    "labels_train = df.sentiment.replace({\"Positivo\": 1, \"Negativo\": 0}).to_list()\n",
    "\n",
    "# corpus_test = x_test.to_list()\n",
    "# labels_test = y_test.replace({\"Positivo\": 1, \"Negativo\": 0}).to_list()\n",
    "\n",
    "# corpus_test = df.tweet_text[:2000].to_list()\n",
    "# labels_test = df.sentiment[:2000].replace({\"Positivo\": 1, \"Negativo\": 0}).to_list()\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words(\"portuguese\") + [\"https\"] + [\"co\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e92b769a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splited_dataset = ShuffleSplit(n_splits=10, test_size=0.2)\n",
    "\n",
    "# splited_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93e5bb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"KNN\": {\n",
    "        \"model_obj\": KNeighborsClassifier(),\n",
    "        \"hyperparameters\": {\n",
    "            \"n_neighbors\": [7, 11, 21],\n",
    "            \"weights\": [\"uniform\", \"distance\"],\n",
    "        },\n",
    "    },\n",
    "    \"SMV\": {\n",
    "        \"model_obj\": svm.SVC(),\n",
    "        \"hyperparameters\": {\n",
    "            \"kernel\": [\"linear\", \"rbf\"],\n",
    "            \"C\": [0.1, 0.5, 1, 5, 10],\n",
    "        },\n",
    "    },\n",
    "    \"GaussianNB\": {\n",
    "        \"model_obj\": GaussianNB(),\n",
    "        \"hyperparameters\": {\n",
    "            \"var_smoothing\": [\n",
    "                1e-8,\n",
    "                1e-6,\n",
    "                1e-4,\n",
    "                1e-2,\n",
    "            ]\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "\n",
    "vectorizers = {\n",
    "    \"TfidfVectorizer\": {\n",
    "        \"vectorizer_obj\": TfidfVectorizer(),\n",
    "        \"hyperparameters\": {\n",
    "            \"max_features\": [500, 1000, 2000],\n",
    "            \"analyzer\": [\"word\", \"char\"],\n",
    "            \"stop_words\": [stop_words, None],\n",
    "            \"tokenizer\": [tweet_tokenizer.tokenize, None],\n",
    "        },\n",
    "    },\n",
    "    \"CountVectorizer\": {\n",
    "        \"vectorizer_obj\": CountVectorizer(),\n",
    "        \"hyperparameters\": {\n",
    "            \"max_features\": [500, 1000, 2000],\n",
    "            \"analyzer\": [\"word\", \"char\"],\n",
    "            \"stop_words\": [stop_words, None],\n",
    "            \"tokenizer\": [tweet_tokenizer.tokenize, None],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "normalizers = {\n",
    "    \"PCA\": {\n",
    "        \"normalizer_obj\": TruncatedSVD(),\n",
    "        \"hyperparameters\": {\n",
    "            \"n_components\": [10, 30, 50, 75],\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "scalers = {\n",
    "    \"Scaler\": {\n",
    "        \"scaler_obj\": StandardScaler(),\n",
    "        \"hyperparameters\": {},\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e4fb298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fiting best model to \n",
      "KNN__TfidfVectorizer__PCA__Scaler\n",
      "\n",
      "Fiting best model to \n",
      "KNN__CountVectorizer__PCA__Scaler\n",
      "\n",
      "Fiting best model to \n",
      "SMV__TfidfVectorizer__PCA__Scaler\n",
      "\n",
      "Fiting best model to \n",
      "SMV__CountVectorizer__PCA__Scaler\n",
      "\n",
      "Fiting best model to \n",
      "GaussianNB__TfidfVectorizer__PCA__Scaler\n",
      "\n",
      "Fiting best model to \n",
      "GaussianNB__CountVectorizer__PCA__Scaler\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_splits_cv = 2\n",
    "n_splits_gs = 2\n",
    "\n",
    "all_scores = {}\n",
    "\n",
    "split_cv = ShuffleSplit(n_splits=n_splits_cv, test_size=0.2)\n",
    "for model_name, model_data in models.items():\n",
    "\n",
    "    model_params = {\n",
    "        f\"model__{key}\": value for key, value in model_data[\"hyperparameters\"].items()\n",
    "    }\n",
    "\n",
    "    for vectorizer_name, vectorizer_data in vectorizers.items():\n",
    "\n",
    "        vectorize_params = {\n",
    "            f\"vectorizer__{key}\": value\n",
    "            for key, value in vectorizer_data[\"hyperparameters\"].items()\n",
    "        }\n",
    "\n",
    "        for normalizer_name, normalizer_data in normalizers.items():\n",
    "\n",
    "            normalizer_params = {\n",
    "                f\"normalizer__{key}\": value\n",
    "                for key, value in normalizer_data[\"hyperparameters\"].items()\n",
    "            }\n",
    "\n",
    "            for scaler_name, scaler_data in scalers.items():\n",
    "\n",
    "                scaler_params = {\n",
    "                    f\"scaler__{key}\": value\n",
    "                    for key, value in scaler_data[\"hyperparameters\"].items()\n",
    "                }\n",
    "\n",
    "                param_distributions = {\n",
    "                    **model_params,\n",
    "                    **vectorize_params,\n",
    "                    **normalizer_params,\n",
    "                    **scaler_params,\n",
    "                }\n",
    "\n",
    "                pipeline = Pipeline(\n",
    "                    steps=[\n",
    "                        (\"vectorizer\", vectorizer_data[\"vectorizer_obj\"]),\n",
    "                        (\"normalizer\", normalizer_data[\"normalizer_obj\"]),\n",
    "                        (\"scaler\", scaler_data[\"scaler_obj\"]),\n",
    "                        (\"model\", model_data[\"model_obj\"]),\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "                approach_name = (\n",
    "                    f\"{model_name}__{vectorizer_name}__{normalizer_name}__{scaler_name}\"\n",
    "                )\n",
    "\n",
    "                print(f\"Fiting best model to \\n{approach_name}\", end=\"\\n\\n\")\n",
    "\n",
    "                tuned_pipeline = RandomizedSearchCV(\n",
    "                    pipeline,\n",
    "                    param_distributions,\n",
    "                    scoring=\"f1\",\n",
    "                    cv=n_splits_gs,\n",
    "                )\n",
    "\n",
    "                scores = cross_validate(\n",
    "                    tuned_pipeline,\n",
    "                    corpus_train,\n",
    "                    labels_train,\n",
    "                    cv=split_cv,\n",
    "                    scoring=[\"accuracy\", \"f1\", \"recall\"],\n",
    "                )\n",
    "\n",
    "                all_scores.update(\n",
    "                    {\n",
    "                        approach_name: {\n",
    "                            \"scores\": scores,\n",
    "                            \"tuned_pipeline\": tuned_pipeline,\n",
    "                        }\n",
    "                    }\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19e49fd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'KNN__TfidfVectorizer__PCA__Scaler': {'scores': {'fit_time': array([14.8744247 , 10.98238444]),\n",
       "   'score_time': array([0.28571343, 0.21477246]),\n",
       "   'test_accuracy': array([0.98      , 0.97833333]),\n",
       "   'test_f1': array([0.96721311, 0.96719933]),\n",
       "   'test_recall': array([0.95848375, 0.95993322])},\n",
       "  'tuned_pipeline': RandomizedSearchCV(cv=2,\n",
       "                     estimator=Pipeline(steps=[('vectorizer', TfidfVectorizer()),\n",
       "                                               ('normalizer', TruncatedSVD()),\n",
       "                                               ('scaler', StandardScaler()),\n",
       "                                               ('model',\n",
       "                                                KNeighborsClassifier())]),\n",
       "                     param_distributions={'model__n_neighbors': [7, 11, 21],\n",
       "                                          'model__weights': ['uniform',\n",
       "                                                             'distance'],\n",
       "                                          'normalizer__n_components': [10, 30, 50,\n",
       "                                                                       75],\n",
       "                                          'vectorizer__analyzer': ['word',\n",
       "                                                                   'char'],\n",
       "                                          'v...\n",
       "                                          'vectorizer__stop_words': [['a', 'à',\n",
       "                                                                      'ao', 'aos',\n",
       "                                                                      'aquela',\n",
       "                                                                      'aquelas',\n",
       "                                                                      'aquele',\n",
       "                                                                      'aqueles',\n",
       "                                                                      'aquilo',\n",
       "                                                                      'as', 'às',\n",
       "                                                                      'até',\n",
       "                                                                      'com',\n",
       "                                                                      'como',\n",
       "                                                                      'da', 'das',\n",
       "                                                                      'de',\n",
       "                                                                      'dela',\n",
       "                                                                      'delas',\n",
       "                                                                      'dele',\n",
       "                                                                      'deles',\n",
       "                                                                      'depois',\n",
       "                                                                      'do', 'dos',\n",
       "                                                                      'e', 'é',\n",
       "                                                                      'ela',\n",
       "                                                                      'elas',\n",
       "                                                                      'ele',\n",
       "                                                                      'eles', ...],\n",
       "                                                                     None],\n",
       "                                          'vectorizer__tokenizer': [<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x7f5a5adbffd0>>,\n",
       "                                                                    None]},\n",
       "                     scoring='f1')},\n",
       " 'KNN__CountVectorizer__PCA__Scaler': {'scores': {'fit_time': array([15.63183713, 13.732867  ]),\n",
       "   'score_time': array([0.18967867, 0.22653437]),\n",
       "   'test_accuracy': array([0.97833333, 0.985     ]),\n",
       "   'test_f1': array([0.96758105, 0.97813765]),\n",
       "   'test_recall': array([0.97487437, 0.97734628])},\n",
       "  'tuned_pipeline': RandomizedSearchCV(cv=2,\n",
       "                     estimator=Pipeline(steps=[('vectorizer', CountVectorizer()),\n",
       "                                               ('normalizer', TruncatedSVD()),\n",
       "                                               ('scaler', StandardScaler()),\n",
       "                                               ('model',\n",
       "                                                KNeighborsClassifier())]),\n",
       "                     param_distributions={'model__n_neighbors': [7, 11, 21],\n",
       "                                          'model__weights': ['uniform',\n",
       "                                                             'distance'],\n",
       "                                          'normalizer__n_components': [10, 30, 50,\n",
       "                                                                       75],\n",
       "                                          'vectorizer__analyzer': ['word',\n",
       "                                                                   'char'],\n",
       "                                          'v...\n",
       "                                          'vectorizer__stop_words': [['a', 'à',\n",
       "                                                                      'ao', 'aos',\n",
       "                                                                      'aquela',\n",
       "                                                                      'aquelas',\n",
       "                                                                      'aquele',\n",
       "                                                                      'aqueles',\n",
       "                                                                      'aquilo',\n",
       "                                                                      'as', 'às',\n",
       "                                                                      'até',\n",
       "                                                                      'com',\n",
       "                                                                      'como',\n",
       "                                                                      'da', 'das',\n",
       "                                                                      'de',\n",
       "                                                                      'dela',\n",
       "                                                                      'delas',\n",
       "                                                                      'dele',\n",
       "                                                                      'deles',\n",
       "                                                                      'depois',\n",
       "                                                                      'do', 'dos',\n",
       "                                                                      'e', 'é',\n",
       "                                                                      'ela',\n",
       "                                                                      'elas',\n",
       "                                                                      'ele',\n",
       "                                                                      'eles', ...],\n",
       "                                                                     None],\n",
       "                                          'vectorizer__tokenizer': [<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x7f5a5adbffd0>>,\n",
       "                                                                    None]},\n",
       "                     scoring='f1')},\n",
       " 'SMV__TfidfVectorizer__PCA__Scaler': {'scores': {'fit_time': array([11.36878777, 23.89521456]),\n",
       "   'score_time': array([0.0521903 , 0.05033612]),\n",
       "   'test_accuracy': array([0.99777778, 0.99722222]),\n",
       "   'test_f1': array([0.99669967, 0.9958368 ]),\n",
       "   'test_recall': array([0.99505766, 0.99500832])},\n",
       "  'tuned_pipeline': RandomizedSearchCV(cv=2,\n",
       "                     estimator=Pipeline(steps=[('vectorizer', TfidfVectorizer()),\n",
       "                                               ('normalizer', TruncatedSVD()),\n",
       "                                               ('scaler', StandardScaler()),\n",
       "                                               ('model', SVC())]),\n",
       "                     param_distributions={'model__C': [0.1, 0.5, 1, 5, 10],\n",
       "                                          'model__kernel': ['linear', 'rbf'],\n",
       "                                          'normalizer__n_components': [10, 30, 50,\n",
       "                                                                       75],\n",
       "                                          'vectorizer__analyzer': ['word',\n",
       "                                                                   'char'],\n",
       "                                          'vectorizer__max_features': [5...\n",
       "                                          'vectorizer__stop_words': [['a', 'à',\n",
       "                                                                      'ao', 'aos',\n",
       "                                                                      'aquela',\n",
       "                                                                      'aquelas',\n",
       "                                                                      'aquele',\n",
       "                                                                      'aqueles',\n",
       "                                                                      'aquilo',\n",
       "                                                                      'as', 'às',\n",
       "                                                                      'até',\n",
       "                                                                      'com',\n",
       "                                                                      'como',\n",
       "                                                                      'da', 'das',\n",
       "                                                                      'de',\n",
       "                                                                      'dela',\n",
       "                                                                      'delas',\n",
       "                                                                      'dele',\n",
       "                                                                      'deles',\n",
       "                                                                      'depois',\n",
       "                                                                      'do', 'dos',\n",
       "                                                                      'e', 'é',\n",
       "                                                                      'ela',\n",
       "                                                                      'elas',\n",
       "                                                                      'ele',\n",
       "                                                                      'eles', ...],\n",
       "                                                                     None],\n",
       "                                          'vectorizer__tokenizer': [<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x7f5a5adbffd0>>,\n",
       "                                                                    None]},\n",
       "                     scoring='f1')},\n",
       " 'SMV__CountVectorizer__PCA__Scaler': {'scores': {'fit_time': array([42.36956191, 41.74280381]),\n",
       "   'score_time': array([0.19951916, 0.04632664]),\n",
       "   'test_accuracy': array([0.98777778, 0.99944444]),\n",
       "   'test_f1': array([0.9801085 , 0.99917695]),\n",
       "   'test_recall': array([0.98545455, 0.99835526])},\n",
       "  'tuned_pipeline': RandomizedSearchCV(cv=2,\n",
       "                     estimator=Pipeline(steps=[('vectorizer', CountVectorizer()),\n",
       "                                               ('normalizer', TruncatedSVD()),\n",
       "                                               ('scaler', StandardScaler()),\n",
       "                                               ('model', SVC())]),\n",
       "                     param_distributions={'model__C': [0.1, 0.5, 1, 5, 10],\n",
       "                                          'model__kernel': ['linear', 'rbf'],\n",
       "                                          'normalizer__n_components': [10, 30, 50,\n",
       "                                                                       75],\n",
       "                                          'vectorizer__analyzer': ['word',\n",
       "                                                                   'char'],\n",
       "                                          'vectorizer__max_features': [5...\n",
       "                                          'vectorizer__stop_words': [['a', 'à',\n",
       "                                                                      'ao', 'aos',\n",
       "                                                                      'aquela',\n",
       "                                                                      'aquelas',\n",
       "                                                                      'aquele',\n",
       "                                                                      'aqueles',\n",
       "                                                                      'aquilo',\n",
       "                                                                      'as', 'às',\n",
       "                                                                      'até',\n",
       "                                                                      'com',\n",
       "                                                                      'como',\n",
       "                                                                      'da', 'das',\n",
       "                                                                      'de',\n",
       "                                                                      'dela',\n",
       "                                                                      'delas',\n",
       "                                                                      'dele',\n",
       "                                                                      'deles',\n",
       "                                                                      'depois',\n",
       "                                                                      'do', 'dos',\n",
       "                                                                      'e', 'é',\n",
       "                                                                      'ela',\n",
       "                                                                      'elas',\n",
       "                                                                      'ele',\n",
       "                                                                      'eles', ...],\n",
       "                                                                     None],\n",
       "                                          'vectorizer__tokenizer': [<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x7f5a5adbffd0>>,\n",
       "                                                                    None]},\n",
       "                     scoring='f1')},\n",
       " 'GaussianNB__TfidfVectorizer__PCA__Scaler': {'scores': {'fit_time': array([7.22828531, 6.66671538]),\n",
       "   'score_time': array([0.15502119, 0.14757276]),\n",
       "   'test_accuracy': array([0.91      , 0.95611111]),\n",
       "   'test_f1': array([0.85663717, 0.93582453]),\n",
       "   'test_recall': array([0.84320557, 0.93053312])},\n",
       "  'tuned_pipeline': RandomizedSearchCV(cv=2,\n",
       "                     estimator=Pipeline(steps=[('vectorizer', TfidfVectorizer()),\n",
       "                                               ('normalizer', TruncatedSVD()),\n",
       "                                               ('scaler', StandardScaler()),\n",
       "                                               ('model', GaussianNB())]),\n",
       "                     param_distributions={'model__var_smoothing': [1e-08, 1e-06,\n",
       "                                                                   0.0001, 0.01],\n",
       "                                          'normalizer__n_components': [10, 30, 50,\n",
       "                                                                       75],\n",
       "                                          'vectorizer__analyzer': ['word',\n",
       "                                                                   'char'],\n",
       "                                          'vectorizer__max_features': [500, 10...\n",
       "                                          'vectorizer__stop_words': [['a', 'à',\n",
       "                                                                      'ao', 'aos',\n",
       "                                                                      'aquela',\n",
       "                                                                      'aquelas',\n",
       "                                                                      'aquele',\n",
       "                                                                      'aqueles',\n",
       "                                                                      'aquilo',\n",
       "                                                                      'as', 'às',\n",
       "                                                                      'até',\n",
       "                                                                      'com',\n",
       "                                                                      'como',\n",
       "                                                                      'da', 'das',\n",
       "                                                                      'de',\n",
       "                                                                      'dela',\n",
       "                                                                      'delas',\n",
       "                                                                      'dele',\n",
       "                                                                      'deles',\n",
       "                                                                      'depois',\n",
       "                                                                      'do', 'dos',\n",
       "                                                                      'e', 'é',\n",
       "                                                                      'ela',\n",
       "                                                                      'elas',\n",
       "                                                                      'ele',\n",
       "                                                                      'eles', ...],\n",
       "                                                                     None],\n",
       "                                          'vectorizer__tokenizer': [<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x7f5a5adbffd0>>,\n",
       "                                                                    None]},\n",
       "                     scoring='f1')},\n",
       " 'GaussianNB__CountVectorizer__PCA__Scaler': {'scores': {'fit_time': array([5.45308733, 5.19429159]),\n",
       "   'score_time': array([0.14622498, 0.15191102]),\n",
       "   'test_accuracy': array([0.91555556, 0.93611111]),\n",
       "   'test_f1': array([0.87396352, 0.90581491]),\n",
       "   'test_recall': array([0.92294221, 0.96006944])},\n",
       "  'tuned_pipeline': RandomizedSearchCV(cv=2,\n",
       "                     estimator=Pipeline(steps=[('vectorizer', CountVectorizer()),\n",
       "                                               ('normalizer', TruncatedSVD()),\n",
       "                                               ('scaler', StandardScaler()),\n",
       "                                               ('model', GaussianNB())]),\n",
       "                     param_distributions={'model__var_smoothing': [1e-08, 1e-06,\n",
       "                                                                   0.0001, 0.01],\n",
       "                                          'normalizer__n_components': [10, 30, 50,\n",
       "                                                                       75],\n",
       "                                          'vectorizer__analyzer': ['word',\n",
       "                                                                   'char'],\n",
       "                                          'vectorizer__max_features': [500, 10...\n",
       "                                          'vectorizer__stop_words': [['a', 'à',\n",
       "                                                                      'ao', 'aos',\n",
       "                                                                      'aquela',\n",
       "                                                                      'aquelas',\n",
       "                                                                      'aquele',\n",
       "                                                                      'aqueles',\n",
       "                                                                      'aquilo',\n",
       "                                                                      'as', 'às',\n",
       "                                                                      'até',\n",
       "                                                                      'com',\n",
       "                                                                      'como',\n",
       "                                                                      'da', 'das',\n",
       "                                                                      'de',\n",
       "                                                                      'dela',\n",
       "                                                                      'delas',\n",
       "                                                                      'dele',\n",
       "                                                                      'deles',\n",
       "                                                                      'depois',\n",
       "                                                                      'do', 'dos',\n",
       "                                                                      'e', 'é',\n",
       "                                                                      'ela',\n",
       "                                                                      'elas',\n",
       "                                                                      'ele',\n",
       "                                                                      'eles', ...],\n",
       "                                                                     None],\n",
       "                                          'vectorizer__tokenizer': [<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x7f5a5adbffd0>>,\n",
       "                                                                    None]},\n",
       "                     scoring='f1')}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f7511e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN__TfidfVectorizer__PCA__Scaler\n",
      "{'fit_time': array([14.8744247 , 10.98238444]), 'score_time': array([0.28571343, 0.21477246]), 'test_accuracy': array([0.98      , 0.97833333]), 'test_f1': array([0.96721311, 0.96719933]), 'test_recall': array([0.95848375, 0.95993322])}\n",
      "\n",
      "\n",
      "KNN__CountVectorizer__PCA__Scaler\n",
      "{'fit_time': array([15.63183713, 13.732867  ]), 'score_time': array([0.18967867, 0.22653437]), 'test_accuracy': array([0.97833333, 0.985     ]), 'test_f1': array([0.96758105, 0.97813765]), 'test_recall': array([0.97487437, 0.97734628])}\n",
      "\n",
      "\n",
      "SMV__TfidfVectorizer__PCA__Scaler\n",
      "{'fit_time': array([11.36878777, 23.89521456]), 'score_time': array([0.0521903 , 0.05033612]), 'test_accuracy': array([0.99777778, 0.99722222]), 'test_f1': array([0.99669967, 0.9958368 ]), 'test_recall': array([0.99505766, 0.99500832])}\n",
      "\n",
      "\n",
      "SMV__CountVectorizer__PCA__Scaler\n",
      "{'fit_time': array([42.36956191, 41.74280381]), 'score_time': array([0.19951916, 0.04632664]), 'test_accuracy': array([0.98777778, 0.99944444]), 'test_f1': array([0.9801085 , 0.99917695]), 'test_recall': array([0.98545455, 0.99835526])}\n",
      "\n",
      "\n",
      "GaussianNB__TfidfVectorizer__PCA__Scaler\n",
      "{'fit_time': array([7.22828531, 6.66671538]), 'score_time': array([0.15502119, 0.14757276]), 'test_accuracy': array([0.91      , 0.95611111]), 'test_f1': array([0.85663717, 0.93582453]), 'test_recall': array([0.84320557, 0.93053312])}\n",
      "\n",
      "\n",
      "GaussianNB__CountVectorizer__PCA__Scaler\n",
      "{'fit_time': array([5.45308733, 5.19429159]), 'score_time': array([0.14622498, 0.15191102]), 'test_accuracy': array([0.91555556, 0.93611111]), 'test_f1': array([0.87396352, 0.90581491]), 'test_recall': array([0.92294221, 0.96006944])}\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>approach</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNN__TfidfVectorizer__PCA__Scaler</td>\n",
       "      <td>12.928405</td>\n",
       "      <td>0.250243</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>0.967206</td>\n",
       "      <td>0.959208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KNN__CountVectorizer__PCA__Scaler</td>\n",
       "      <td>14.682352</td>\n",
       "      <td>0.208107</td>\n",
       "      <td>0.981667</td>\n",
       "      <td>0.972859</td>\n",
       "      <td>0.976110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SMV__TfidfVectorizer__PCA__Scaler</td>\n",
       "      <td>17.632001</td>\n",
       "      <td>0.051263</td>\n",
       "      <td>0.997500</td>\n",
       "      <td>0.996268</td>\n",
       "      <td>0.995033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SMV__CountVectorizer__PCA__Scaler</td>\n",
       "      <td>42.056183</td>\n",
       "      <td>0.122923</td>\n",
       "      <td>0.993611</td>\n",
       "      <td>0.989643</td>\n",
       "      <td>0.991905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GaussianNB__TfidfVectorizer__PCA__Scaler</td>\n",
       "      <td>6.947500</td>\n",
       "      <td>0.151297</td>\n",
       "      <td>0.933056</td>\n",
       "      <td>0.896231</td>\n",
       "      <td>0.886869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GaussianNB__CountVectorizer__PCA__Scaler</td>\n",
       "      <td>5.323689</td>\n",
       "      <td>0.149068</td>\n",
       "      <td>0.925833</td>\n",
       "      <td>0.889889</td>\n",
       "      <td>0.941506</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   approach   fit_time  score_time  accuracy  \\\n",
       "0         KNN__TfidfVectorizer__PCA__Scaler  12.928405    0.250243  0.979167   \n",
       "1         KNN__CountVectorizer__PCA__Scaler  14.682352    0.208107  0.981667   \n",
       "2         SMV__TfidfVectorizer__PCA__Scaler  17.632001    0.051263  0.997500   \n",
       "3         SMV__CountVectorizer__PCA__Scaler  42.056183    0.122923  0.993611   \n",
       "4  GaussianNB__TfidfVectorizer__PCA__Scaler   6.947500    0.151297  0.933056   \n",
       "5  GaussianNB__CountVectorizer__PCA__Scaler   5.323689    0.149068  0.925833   \n",
       "\n",
       "         f1    recall  \n",
       "0  0.967206  0.959208  \n",
       "1  0.972859  0.976110  \n",
       "2  0.996268  0.995033  \n",
       "3  0.989643  0.991905  \n",
       "4  0.896231  0.886869  \n",
       "5  0.889889  0.941506  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_scores\n",
    "\n",
    "corpus_name = \"NoThemeTweets\"\n",
    "approach_names = []\n",
    "fit_times = []\n",
    "scores_times = []\n",
    "accuracy_means = []\n",
    "f1_scores_mean = []\n",
    "recall_scores_mean = []\n",
    "\n",
    "\n",
    "for approach_name, score in all_scores.items():\n",
    "    print(f\"{approach_name}\")\n",
    "    print(f\"{score['scores']}\")\n",
    "    approach_names.append(approach_name)\n",
    "    fit_times.append(score[\"scores\"][\"fit_time\"].mean())\n",
    "    scores_times.append(score[\"scores\"][\"score_time\"].mean())\n",
    "    accuracy_means.append(score[\"scores\"][\"test_accuracy\"].mean())\n",
    "    f1_scores_mean.append(score[\"scores\"][\"test_f1\"].mean())\n",
    "    recall_scores_mean.append(score[\"scores\"][\"test_recall\"].mean())\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "# data={'Name':['Karan','Rohit','Sahil','Aryan'],'Age':[23,22,21,24]}\n",
    "\n",
    "\n",
    "test_data = data = {\n",
    "    \"approach\": approach_names,\n",
    "    \"fit_time\": fit_times,\n",
    "    \"score_time\": scores_times,\n",
    "    \"accuracy\": accuracy_means,\n",
    "    \"f1\": f1_scores_mean,\n",
    "    \"recall\": recall_scores_mean,\n",
    "}\n",
    "\n",
    "\n",
    "test_data_df = pd.DataFrame(test_data)\n",
    "\n",
    "\n",
    "test_data_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e259b5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(df.tweet_text,df.sentiment,test_size=0.2, random_state=42)\n",
    "\n",
    "corpus_train2 = x_train.to_list()\n",
    "labels_train2 = y_train.replace({\"Positivo\": 1, \"Negativo\": 0}).to_list()\n",
    "\n",
    "corpus_test2 = x_test.to_list()\n",
    "labels_test2 = y_test.replace({\"Positivo\": 1, \"Negativo\": 0}).to_list()\n",
    "\n",
    "\n",
    "all_scores[\"SMV__TfidfVectorizer__PCA__Scaler\"][\"tuned_pipeline\"].fit(corpus_train2, labels_train2)\n",
    "\n",
    "print(\"______\" * 30)\n",
    "print(all_scores[\"SMV__TfidfVectorizer__PCA__Scaler\"][\"tuned_pipeline\"].best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e0c7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = all_scores[\"SMV__TfidfVectorizer__PCA__Scaler\"][\"tuned_pipeline\"].predict(corpus_test2)\n",
    "\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f00716",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(labels_test2, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1761497",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(labels_test2, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79654fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = all_scores[\" KNN + TfidfVectorizer + PCA + Scaler\"][\"pipeline\"].best_params_\n",
    "a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da5657c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627aae4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits_cv = 2\n",
    "n_splits_gs = 2\n",
    "\n",
    "all_scores = {}\n",
    "\n",
    "split_cv = ShuffleSplit(n_splits=n_splits_cv, test_size=0.2)\n",
    "\n",
    "for model_name, model_data in models.items():\n",
    "\n",
    "    model_params = {\n",
    "        f\"model__{key}\": value for key, value in model_data[\"hyperparameters\"].items()\n",
    "    }\n",
    "    print(model_params)\n",
    "\n",
    "    param_distributions = {\n",
    "        \"vectorizer__tfidf__use_idf\": [False, True],\n",
    "        \"vectorizer__count__max_features\": [1000, 2000],\n",
    "        \"pca__n_components\": [100, 200, 500],\n",
    "        **model_params,\n",
    "    }\n",
    "\n",
    "    print(param_distributions)\n",
    "\n",
    "    pipeline = Pipeline(\n",
    "        steps=[\n",
    "            (\"vectorizer\", vectorizer),\n",
    "            (\"pca\", pca),\n",
    "            (\"normalize\", scaler),\n",
    "            (\"model\", model_data[\"model_obj\"]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    gs_model = RandomizedSearchCV(\n",
    "        pipeline,\n",
    "        param_distributions,\n",
    "        scoring=\"f1\",\n",
    "        cv=n_splits_gs,\n",
    "    )\n",
    "\n",
    "    scores = cross_validate(\n",
    "        gs_model, corpus, labels, cv=split_cv, scoring=[\"accuracy\", \"f1\"]\n",
    "    )\n",
    "\n",
    "    all_scores.update({model_name: scores})\n",
    "\n",
    "scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6579eb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae3498d",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_distributions = {\n",
    "    \"vectorizer__n_components\": [100, 200, 500, 1000],\n",
    "    **model_data[\"hyperparameters\"],\n",
    "}\n",
    "\n",
    "param_distributions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6661ef-f76d-477f-8b8d-c2ca4bf2a3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\n",
    "            \"vectorizer\",\n",
    "            CountVectorizer(analyzer=\"word\", stop_words=stop_words, max_features=1000),\n",
    "        ),\n",
    "        (\"pca\", TruncatedSVD(500)),\n",
    "        (\"clf\", KNeighborsClassifier(n_neighbors=11)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline2 = Pipeline(\n",
    "    steps=[\n",
    "        (\n",
    "            \"vectorizer\",\n",
    "            CountVectorizer(\n",
    "                analyzer=\"word\",\n",
    "                stop_words=stop_words,\n",
    "                max_features=1000,\n",
    "                tokenizer=tweet_tokenizer.tokenize,\n",
    "            ),\n",
    "        ),\n",
    "        (\"pca\", TruncatedSVD(500)),\n",
    "        (\"clf\", KNeighborsClassifier(n_neighbors=11)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "pipeline3 = Pipeline(\n",
    "    steps=[\n",
    "        (\n",
    "            \"vectorizer\",\n",
    "            CountVectorizer(\n",
    "                analyzer=\"word\",\n",
    "                stop_words=stop_words,\n",
    "                max_features=1000,\n",
    "                tokenizer=tweet_tokenizer.tokenize,\n",
    "            ),\n",
    "        ),\n",
    "        (\"clf\", MultinomialNB()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "pipeline4 = Pipeline(\n",
    "    steps=[\n",
    "        (\n",
    "            \"vectorizer\",\n",
    "            CountVectorizer(\n",
    "                analyzer=\"word\",\n",
    "                stop_words=stop_words,\n",
    "                max_features=1000,\n",
    "                tokenizer=tweet_tokenizer.tokenize,\n",
    "            ),\n",
    "        ),\n",
    "        (\"clf\", svm.SVC(kernel=\"linear\")),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f42487-c822-4e5f-b13f-990b8d292df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(corpus_train, labels_train)\n",
    "pipeline2.fit(corpus_train, labels_train)\n",
    "pipeline3.fit(corpus_train, labels_train)\n",
    "pipeline4.fit(corpus_train, labels_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6580cedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline3.fit(corpus_train, labels_train)\n",
    "# y_hat3 = pipeline3.predict(corpus_test)\n",
    "# y_hat3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48cbfaa-6878-4213-83b3-4c0cee16b3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = pipeline.predict(corpus_test)\n",
    "y_hat2 = pipeline2.predict(corpus_test)\n",
    "y_hat3 = pipeline3.predict(corpus_test)\n",
    "y_hat4 = pipeline4.predict(corpus_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4478eb-e530-4a30-8eb4-8e713414fd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(labels_test, y_hat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2540a76-3869-4529-a506-72aacf0a10a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(labels_test, y_hat2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843da6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(labels_test, y_hat3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e826a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(labels_test, y_hat4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bf013e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "\n",
    "\n",
    "cv = CountVectorizer(\n",
    "    analyzer=\"word\",\n",
    "    stop_words=stop_words,\n",
    "    max_features=1500,\n",
    "    tokenizer=tweet_tokenizer.tokenize,\n",
    ")\n",
    "\n",
    "freq_train = cv.fit_transform(corpus_train)\n",
    "\n",
    "\n",
    "freq_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d7872e-c3d4-4789-9623-ad8873e39986",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model = MultinomialNB()\n",
    "\n",
    "model.fit(freq_train, labels_train)\n",
    "\n",
    "model\n",
    "\n",
    "\n",
    "freq_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b39f7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "teste = [\n",
    "    \"Não é um bom dia para começar a programar\",\n",
    "    \"Estou ficando muito cansado\",\n",
    "    \"Não consigo fazer nada\",\n",
    "    \"Que dia lindo, hoje é dia de programar\",\n",
    "    \"É coisa, é coisa pura, é coisa maravilhosa\",\n",
    "    \"Bolsonaro é um cara horrível\",\n",
    "    \"Estou querendo morrer\",\n",
    "    \"Abençoa senhor a minha vida bolsonaro\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c48f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados = cross_val_predict(gs_model, corpus, labels, cv=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8bd963",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados2 = cross_val_score(pipeline4, corpus_train, labels_train, cv=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6b9731",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.accuracy_score(labels_train, resultados)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aeac9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados3 = cross_validate(pipeline4, corpus_train, labels_train, cv=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f090c65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados3\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('src-zwQccrdW-py3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "882c8c6501720d7339a894262ca6c76e47c685bc126574a2b14e77f6df77b0f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
